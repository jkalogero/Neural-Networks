{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Lab_1.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgM-C62wlQTm"
      },
      "source": [
        "---\n",
        "#Άσκηση 1. Επιβλεπόμενη Μάθηση: Ταξινόμηση\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTjsQhI7orML"
      },
      "source": [
        "#Στοιχεία Ομάδας\n",
        "Συνεργάτες:\n",
        "Δούλης Κωνσταντίνος 03116175\n",
        "\n",
        "Καλογερόπουλος Ιωάννης 03116117\n",
        "\n",
        "Κατσίκας Μουρούτσος Γεώργιος 03116132\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik7SPquAmwND"
      },
      "source": [
        "Αρχικά, ενημερώνουμε τις βιβλιοθήκες που θα χρησιμοποιηθούν (έχει προστεθεί στην αρχή του κελιού η magic command %%capture ώστε να μην εμφανίζονται στο stdouput οι πληροφορίες των εγκαταστάσεων, για να είναι πιο ευανάγνωστο):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7_jP0HklpR1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "46dcb40c-11b1-4a72-d888-5f5984a8bb86"
      },
      "source": [
        "!pip install --upgrade pip #upgrade pip package installer\n",
        "!pip install scikit-learn --upgrade #upgrade scikit-learn package\n",
        "!pip install numpy --upgrade #upgrade numpy package\n",
        "!pip install pandas --upgrade #--upgrade #upgrade pandas package\n",
        "!pip install -U tensorflow\n",
        "!pip install --upgrade imbalanced-learn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pip in /usr/local/lib/python3.6/dist-packages (20.2.4)\n",
            "Requirement already up-to-date: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.23.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.17.0)\n",
            "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (2.1.0)\n",
            "Collecting numpy\n",
            "  Using cached numpy-1.19.4-cp36-cp36m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
            "\n",
            "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
            "\n",
            "tensorflow 2.3.1 requires numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.4 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.19.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pandas in /usr/local/lib/python3.6/dist-packages (1.1.4)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.19.4)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already up-to-date: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.33.2)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.35.1)\n",
            "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Collecting numpy<1.19.0,>=1.16.0\n",
            "  Using cached numpy-1.18.5-cp36-cp36m-manylinux1_x86_64.whl (20.1 MB)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow) (50.3.2)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.2)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3.3)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.11.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.4\n",
            "    Uninstalling numpy-1.19.4:\n",
            "      Successfully uninstalled numpy-1.19.4\n",
            "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
            "\n",
            "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
            "\n",
            "datascience 0.10.6 requires folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.18.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: imbalanced-learn in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (0.17.0)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.23 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (0.23.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.23->imbalanced-learn) (2.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUYHWBBvjb9w"
      },
      "source": [
        "import warnings \n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9L1ZXDKncGu"
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "def download(url, file):\n",
        "    if not os.path.isfile(file):\n",
        "        urlretrieve(url,file)\n",
        "        print(\"File downloaded\")\n",
        "\n",
        "download('http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data', 'crx.data')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuojFZSVqK3y"
      },
      "source": [
        "#Mικρό Dataset: Japanese Credit Screening\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q2QlhnxrYXY"
      },
      "source": [
        "##Πληροφορίες dataset\n",
        "\n",
        "Το dataset που εξετάζουμε παρέχει πληροφορίες σχετικά με άτομα στα οποία χορηγήθηκε ή όχι πίστωση από Ιαπωνική εταιρεία. Τα δεδομένα παράχθηκαν ύστερα από σχετικές ερωτήσεις στους πελάτες τις εταιρείας."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAMbop8Pr0B8"
      },
      "source": [
        "##Περιγραφή χαρακτηριστικών του dataset\n",
        "\n",
        "To dataset διαθέτει <b>690 δείγματα</b>, με το καθένα από αυτά να διαθέτει <b>15 χαρακτηριστικά συν 1 που είναι η κλάση</b>.\n",
        "\n",
        "Το κάθε δείγμα αντιστοιχεί σε μία γραμμή του αρχείου, ενώ η κάθε στήλη σε ένα χαρακτηριστικό του. <b>Η τελευταία στήλη δηλώνει την κλάση που ανήκει το δείγμα</b>. Συγκεκριμένα, το σύμβολο <b>\"+\"</b> δηλώνει ότι η πίστωση <b>εγκρίθηκε</b>, ενώ το σύμβολο <b>\"-\"</b> δηλώνει ότι <b>δεν εγκρίθηκε</b>.\n",
        "Επομένως, η δομή του αρχείου είναι κατάλληλη για να ξεκινήσουμε την προεπεξεργασία του dataset.\n",
        "\n",
        "Για λόγους απορήτου τα attribute names και values έχουν αντικατασταθεί με τυχαία σύμβολα, που δεν παρέχουν κάποια σχετική πληροφορία σχετικά με το τι περιγράφουν.\n",
        "\n",
        "Το είδος των χαρακτηριστικών φαίνεται παρακάτω:\n",
        "\n",
        "| Attribute | Type of Data                                 | Description    |\n",
        "|-----------|----------------------------------------------|----------------|\n",
        "| A1        | b, a.                                        | Male           |\n",
        "| A2        | continuous.                                  | Age            |\n",
        "| A3        | continuous.                                  | Debt           |\n",
        "| A4        | u, y, l, t.                                  | Married        |\n",
        "| A5        | g, p, gg.                                    | BankCustomer   |\n",
        "| A6        | c, d, cc, i, j, k, m, r, q, w, x, e, aa, ff. | EducationLevel |\n",
        "| A7        | v, h, bb, j, n, z, dd, ff, o.                | Ethnicity      |\n",
        "| A8        | continuous.                                  | YearsEmployed  |\n",
        "| A9        | t, f.                                        | PriorDefault   |\n",
        "| A10       | t, f.                                        | Employed       |\n",
        "| A11       | continuous.                                  | CreditScore    |\n",
        "| A12       | t, f.                                        | DriversLicense |\n",
        "| A13       | g, p, s.                                     | Citizen        |\n",
        "| A14       | continuous.                                  | ZipCode        |\n",
        "| A15       | continuous.                                  | Income         |\n",
        "| A16       | +,-                                          | Approved (CLASS ATTRIBUTE)       |\n",
        "\n",
        "\n",
        "\n",
        "* Τα χαρακτηριστικά που έχουν continuous τιμές αποτελουν <b>διατεταγμένα </b>χαρακτηριστικά, ενώ αυτά που έχουν διακριτές τιμές, δηλαδή σύμβολα, <b>μη διατεταγμένα</b>.\n",
        "\n",
        "* Στο dataset <b>δεν</b> υπάρχουν επικεφαλίδες, καθώς ούτε και αρίθμηση γραμμών.\n",
        "\n",
        "* Οι ετικέτες και οι σημασίες τους είναι:\n",
        "    * \"+\": Εγκρίθηκε η πίστωση\n",
        "    * \"-\": Απορρίφθηκε η πίστωση\n",
        "\n",
        "    Οι ετικέτες αυτές βρίσκονται στην τελευταία στήλη, ενώ οι τιμές τους θα πρέπει να μετατραπούν όπως φαίνεται παρακάτω:\n",
        "    * \"+\" -> 1\n",
        "    * \"-\" -> 0\n",
        "\n",
        "* <b>Υπάρχουν απουσιάζουσες τιμές</b>. Συγκεκριμένα, 37 δείγματα (5%) του συνολικού αριθμού δειγμάτων παρουσιάζουν μία ή παραπάνω απώλειες δεδομένων. \n",
        "\n",
        "Όλα τα παραπάνω φαίνονται παρακάτω:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW4faeHk8TKM"
      },
      "source": [
        "##Προεπεξεργασία των δεδομένων\n",
        "Aρχικά διαβάζουμε το αρχείο <b>crx.data</b>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNOetXVa77M7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0550231d-c0ff-41ba-9160-991f65d28e76"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"crx.data\", header=None)\n",
        "# print the five first samples\n",
        "print(df[:5])\n",
        "print(df.shape)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0      1      2  3  4  5  6     7  8  9   10 11 12     13   14 15\n",
            "0  b  30.83  0.000  u  g  w  v  1.25  t  t   1  f  g  00202    0  +\n",
            "1  a  58.67  4.460  u  g  q  h  3.04  t  t   6  f  g  00043  560  +\n",
            "2  a  24.50  0.500  u  g  q  h  1.50  t  f   0  f  g  00280  824  +\n",
            "3  b  27.83  1.540  u  g  w  v  3.75  t  t   5  t  g  00100    3  +\n",
            "4  b  20.17  5.625  u  g  w  v  1.71  t  f   0  f  s  00120    0  +\n",
            "(690, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdZtApI_9Fab"
      },
      "source": [
        "###Αντιστοίχιση κλάσεων\n",
        "Παρατηρούμε, όπως ειπώθηκε και παραπάνω, ότι οι κλάσεις δηλώνονται στην τελευταία στήλη με τα σύμβολα +/-. \n",
        "\n",
        "Μετατρέπουμε τα \"+\" σε 1 και τα \"-\" σε 0:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3M1H7ST9O9x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7ae6029-df9f-4767-d7a5-7e11c32719c0"
      },
      "source": [
        "# create mapper for each symbol\n",
        "class_mapper = {\"+\": 1, \"-\": 0}\n",
        "# since these symbols occur only on the last column we dont specify the column\n",
        "df = df.replace(class_mapper)\n",
        "print(df[:5])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0      1      2  3  4  5  6     7  8  9   10 11 12     13   14  15\n",
            "0  b  30.83  0.000  u  g  w  v  1.25  t  t   1  f  g  00202    0   1\n",
            "1  a  58.67  4.460  u  g  q  h  3.04  t  t   6  f  g  00043  560   1\n",
            "2  a  24.50  0.500  u  g  q  h  1.50  t  f   0  f  g  00280  824   1\n",
            "3  b  27.83  1.540  u  g  w  v  3.75  t  t   5  t  g  00100    3   1\n",
            "4  b  20.17  5.625  u  g  w  v  1.71  t  f   0  f  s  00120    0   1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p35rb4qAuRYR"
      },
      "source": [
        "###Εντοπισμός απουσιάζουσων τιμών χαρακτηριστικών\n",
        "\n",
        "Στη συνέχεια, θα εντοπίσουμε τις απουσιάζουσες τιμές. Αυτές δηλώνονται στο dataset με τον χαρακτήρα \"?\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4snaGwf-KV3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "676637c3-6b42-4de2-dbf7-27c3be56f3c1"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# replace \"?\" with np.NaN\n",
        "df.replace('?',np.NaN,inplace=True)\n",
        "\n",
        "# calculate the number of samples with at least one missing attribute\n",
        "# df.isna() is the mask of df where each element is True if is NaN\n",
        "# s is the length of the list of samples that have at least one True(NaN)\n",
        "num_of_incomplete_samples = len([i for i in np.array(df.isna()) if True in i])\n",
        "\n",
        "print(\"The samples of the dataset that have at least one missing attribute are \", num_of_incomplete_samples)\n",
        "print(\"Which means that the \", num_of_incomplete_samples*100/df.shape[0], \"% of the samples have missing values.\", sep=\"\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The samples of the dataset that have at least one missing attribute are  37\n",
            "Which means that the 5.36231884057971% of the samples have missing values.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdHVwDC30eRv"
      },
      "source": [
        "Από τα παραπάνω βλέπουμε ότι αν θέλαμε να αγνοήσουμε τα δείγματα που έχουν missing values θα έπρεπε να \"πετάξουμε\" το 5% των δειγμάτων, αριθμός που <b>θα επηρέαζε αρνητικά τα αποτελέσματα</b>.\n",
        "\n",
        "Προκειμένου να εξετάσουμε αν οι περισσότερες τιμές που λείπουν προέρχονται απο συγκεκριμένα (λίγα) χαρακτηριστικά (και αν αυτά έχουν ίσως και μηδενική διακύμανση, δηλαδή  χαρακτηριστικά με σταθερές τιμές), οπότε θα μπορούσαμε να τα αγνοήσουμε, εκτελούμε:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkPRNBdi32T1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4b6963f-2b3b-4630-ed05-ed220491c783"
      },
      "source": [
        "# the list of the number of missing values for each attribute occurs by \n",
        "# summing the elements of the inverse of df, without df's last column.\n",
        "# df's last column is the class attribute and it is alwasy present.\n",
        "incomplete_attrs = [sum(i) for i in np.array(df.isna())[:,:df.shape[1]-1].T]\n",
        "print(\"For each attribute of the dataset, the number of the missing values is\")\n",
        "print(incomplete_attrs)\n",
        "# print(sum(incomplete_attrs))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For each attribute of the dataset, the number of the missing values is\n",
            "[12, 12, 0, 6, 6, 9, 9, 0, 0, 0, 0, 0, 0, 13, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fXm2-b_32zj"
      },
      "source": [
        "Βλέπουμε, επομένως, ότι από τα 690 δείγματα το πολύ σε 13 από αυτά ένα χαρακτηριστικό δεν έχει τιμές. Επομένως, δεν έχει νόημα να αγνοήσουμε το χαρακτηριστικό αυτό, ακόμα και αν είναι αυτό με τις περισσότερες ελλείψεις, αφού αυτές είναι πολύ λίγες και θα χάναμε πολύ πληροφορία.\n",
        "\n",
        "Τελικά η πιο συμφέρουσα λύση είναι να αντικαταστήσουμε τα NaN με τιμές που προκύπτουν με βάση τις τιμές των χαρακτηριστικών σε προηγούμενα δείγματα. <u>Συγκεκριμένα, αντικαθιστούμε τα NaN με την τιμή του χαρακτηριστικού που εμφανίζεται πιο συχνά</u>.\n",
        "\n",
        "Την αντικατάσταση των απουσιάζουσων τιμών θα τη χειρίζεται το πρώτο κομμάτι προεπεξεργασίας του Pipeline, που θα οριστεί στη συνέχεια και θα πρόκειται για έναν μετασχηματιστή Imputer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iIfeNp-F20h"
      },
      "source": [
        "Οι κλάσεις βρίσκονται στην τελευταία στήλη του συνόλου δεδομένων. Οπότε για να βρούμε το πλήθος τους και τα ποσοστά δειγμάτων τους επί του συνόλου:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc32MgrVGpPE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8147d75-43dc-42f5-c766-3eba5bc5598a"
      },
      "source": [
        "num_of_rows = df.shape[0]\n",
        "num_of_attrs = df.shape[1] - 1 #remove one element because of the class attribute\n",
        "\n",
        "# get labesl and features\n",
        "labels_df = df.iloc[:, [num_of_attrs]] # τα labels είναι στην τελευταία κολώνα\n",
        "features_df = df.iloc[:, 0:num_of_attrs]  # τα features είναι όλες οι προηγούμενες κολώνες\n",
        "\n",
        "labels = labels_df.values.reshape(num_of_rows,)\n",
        "features = features_df.values\n",
        "# convert to int\n",
        "labels.astype(int)\n",
        "labels = np.array(labels, dtype='int64')\n",
        "# print(labels.shape)\n",
        "\n",
        "# find how many of each class\n",
        "bin_count = np.bincount(labels)\n",
        "print (\"frequencies:\", bin_count)\n",
        "print(\"The percentage of 0's in data: \", bin_count[0]*100/sum(bin_count), \"%.\")\n",
        "print(\"The percentage of 1's in data: \", bin_count[1]*100/sum(bin_count), \"%.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frequencies: [383 307]\n",
            "The percentage of 0's in data:  55.507246376811594 %.\n",
            "The percentage of 1's in data:  44.492753623188406 %.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW0hDxy-LS28"
      },
      "source": [
        "Καταλήγουμε, λοιπόν, ότι τα ποσοστά είναι περίπου 55.5% για την κλάση 0 και 44.5% για την κλάση 1. Επομένως, το dataset είναι <b>ισορροπημένο</b>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqMTLEANH4ST",
        "outputId": "00750a04-090e-4603-d9b0-93b4b5034ca1"
      },
      "source": [
        "# print(labels[:5])\n",
        "print(features.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(690, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe7knthjExus"
      },
      "source": [
        "###Εντοπισμός κατηγορικών χαρακτηριστικών\n",
        "Το dataset περιλαμβάνει εκτός από χαρακτηριστικά με συνεχείς αριθμητικές τιμές και κατηγορικά χαρακτηριστικά. Από τις πληροφορίες που έχουμε για το dataset, τα χαρακτηριστικά που είναι κατηγορικά είναι αυτά που αντιστοιχούν στις στήλες:\n",
        "\n",
        "    0,3,4,5,6,8,9,11,12\n",
        "\n",
        "Ο χειρισμός των κατηγορικών συμβόλων θα γίνει μετά το χειρισμό των απουσιάζουσων τιμών και πριν το Cross Validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yni1P1F6D-89"
      },
      "source": [
        "##Διαχωρισμός του dataset\n",
        "Διαχωρίζουμε το dataset σε train και test set, χρησιμοποιώντας το 80% των δεδομένων για το training και το 20% για το testing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hze8obkLEg0K"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "train, test, train_labels, test_labels = train_test_split(features, labels, test_size=0.2, random_state=78)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOLwbo9dBD4a"
      },
      "source": [
        "train_pd = pd.DataFrame(data=train[:,:],    # values\n",
        "                 index=train[:,0])    # 1st column as index\n",
        "                  \n",
        "test_pd = pd.DataFrame(data=test[:,:],    # values\n",
        "                 index=test[:,0])    # 1st column as index"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrNfyxLaBd8Y",
        "outputId": "58ba95bf-2aa3-43a2-ae32-983489c514b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(train.shape)\n",
        "print(train_pd.shape)\n",
        "\n",
        "print(test.shape)\n",
        "print(test_pd.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(552, 15)\n",
            "(552, 15)\n",
            "(138, 15)\n",
            "(138, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo0VGdmqlTjd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17baa981-2cf8-492e-f846-c14fbc38db4e"
      },
      "source": [
        "from collections import defaultdict\n",
        "# just for checking...\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "check = defaultdict(int)\n",
        "for i in range(0,len(train_labels)):\n",
        "    check['pos_train'] += train_labels[i] == 1\n",
        "    check['neg_train'] += train_labels[i] == 0\n",
        "    \n",
        "\n",
        "# for i in range(0,len(test_labels)):\n",
        "#     check['pos_test'] += test_labels[i] == 1\n",
        "#     check['neg_test'] += test_labels[i] == 0\n",
        "    \n",
        "# print(check)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(552, 15)\n",
            "(138, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6hs-A0Q5a1j"
      },
      "source": [
        "##Ορισμός μετασχηματιστών"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5QAIBec92x_"
      },
      "source": [
        "###Variance Threshold\n",
        "\n",
        "Παρακάτω υλοποιείται η συνάρτηση που αρχικοποιεί selector Variance Threshold, με κατόφλι που δίνεται από το όρισμα threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jV9JVV6X68pn"
      },
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "def Variance_Threshold(data, threshold=0):\n",
        "    # αρχικοποιούμε έναν selector\n",
        "    train, test, train_labels, test_labels = data\n",
        "    selector = VarianceThreshold(threshold = threshold)\n",
        "    train_reduced = selector.fit_transform(train)\n",
        "    test_reduced = selector.transform(test)\n",
        "\n",
        "    return (train_reduced, test_reduced, train_labels, test_labels)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug166UU_88B1"
      },
      "source": [
        "###PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHoEFrk689m2"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def PCA_selector(data, n):\n",
        "    \"\"\"\n",
        "    Function that creates a PCA selector that learns the train data\n",
        "    and transforms train and test data.\n",
        "    \"\"\"\n",
        "    train, test, train_labels, test_labels = data\n",
        "    # αρχικοποιούμε τον selector\n",
        "    pca = PCA(n_components=n)\n",
        "    trainPCA = pca.fit_transform(train)\n",
        "    testPCA = pca.transform(test)\n",
        "\n",
        "    return (trainPCA, testPCA, train_labels, test_labels)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2gtTLCt9zko"
      },
      "source": [
        "###Κανονικοποίηση"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIz6YOlw-KEu"
      },
      "source": [
        "from scipy import stats as st\n",
        "\n",
        "def StandarizerTransformer(data, method):\n",
        "    \"\"\"\n",
        "    Function that standarizes the data using two different methods.\n",
        "    minmax and zscore.    \n",
        "    \"\"\"\n",
        "    train, test, train_labels, test_labels = data\n",
        "    if method==\"zscore\":\n",
        "        std_train = st.zscore(train)\n",
        "    elif method==\"minmax\":\n",
        "        std_train = (train - np.min(train) )/ (np.max(train) - np.min(train))\n",
        "    else:\n",
        "        std_train = train\n",
        "    return (std_train, test, train_labels, test_labels)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1qXIMpk_x--"
      },
      "source": [
        "###Εξισορρόπηση dataset - Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QaC5aRuVZlL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89197743-1bd9-4dba-ef3e-0112005185c7"
      },
      "source": [
        "!pip install six"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLjaejVoANn9"
      },
      "source": [
        "# from sklearn.externals import six\n",
        "# from imblearn.over_sampling import RandomOverSampler\n",
        "# from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "def sampling(data, method):\n",
        "    train, test, train_labels, test_labels = data\n",
        "    if method == \"over\":\n",
        "        ros = RandomOverSampler()\n",
        "        # mlb = MultiLabelBinarizer().fit(['0', '1', '2', '3', '4'])\n",
        "        # tmplabels = np.asarray(mlb.inverse_transform(C_trainDataTargets)).flatten()\n",
        "        train_resampled, train_labels_resampled = ros.fit_sample(train,train_labels)\n",
        "        trainTargets_resampled = mlb.transform(trainTargets_resampled)\n",
        "    elif method == 'under':\n",
        "        ros = RandomUnderSampler()\n",
        "        # mlb = MultiLabelBinarizer().fit(['0', '1', '2', '3', '4'])\n",
        "        # tmplabels = np.asarray(mlb.inverse_transform(C_trainDataTargets)).flatten()\n",
        "        train_resampled, train_labels_resampled = ros.fit_sample(train,train_labels)\n",
        "        trainTargets_resampled = mlb.transform(trainTargets_resampled)\n",
        "    \n",
        "    return (train_resampled, test, train_labels_resampled, test_labels)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvnXfmKlVr06"
      },
      "source": [
        "##Ορισμός ταξινομητών"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIZND57tVwK0"
      },
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n",
        "def createClassifier(classifier, argument=5):\n",
        "\n",
        "    if classifier == 'dummy':\n",
        "        clf = DummyClassifier(strategy=argument)\n",
        "    elif classifier == 'gnb':\n",
        "        clf = GaussianNB()\n",
        "    elif classifier == 'knn':\n",
        "        clf = KNeighborsClassifier(n_neighbors=argument)\n",
        "    \n",
        "    return clf"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aZdMfMTZGUE"
      },
      "source": [
        "##Ορισμός Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCPJcKB9ZJm0"
      },
      "source": [
        "def Pipeline (data, steps):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        * data: (train, train_labels, test, test_labels)\n",
        "        * steps: list of tuples, where each tuple's 1st el \n",
        "        is the transformer/classifier and the 2nd el\n",
        "        is the arguments of the function for the\n",
        "        transformer/classifier\n",
        "    \"\"\"\n",
        "\n",
        "    mapper = {\n",
        "        'selector': Variance_Threshold,\n",
        "        'scaler': StandarizerTransformer,\n",
        "        'sampler': sampling,\n",
        "        'pca': PCA_selector,\n",
        "        'kNN': createClassifier,\n",
        "        'dummy': createClassifier\n",
        "    }\n",
        "\n",
        "    classifiers = set(['kNN', 'uniform', 'constant_0', 'constant_1', 'most_frequent', 'stratified'])\n",
        "\n",
        "    for step in steps:\n",
        "        if step[0] not in classifiers:\n",
        "            # update data\n",
        "            data = mapper[step[0]](step[1])\n",
        "\n",
        "        else:\n",
        "            # return model\n",
        "            return (data, createClassifier(step[0], step[1]))\n",
        "\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Myc4SzW_5CKC"
      },
      "source": [
        "##Κατασκευή Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNwAJkIN_XGK"
      },
      "source": [
        "###Χειρισμός απουσιάζουσων τιμών\n",
        "Όπως καταλήξαμε θα χρησιμοποιηθεί ο μετασχηματιστής Imputer, για τον χειρισμό των απουσιάζουσων τιμών, θέτοντας σε αυτές την πιο συχνή τιμή του χαρακτηριστικού"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-EJG3JR_wjD"
      },
      "source": [
        "# create imputer that will replace NaN with the most frequent value\n",
        "imp=SimpleImputer(missing_values=np.NaN,strategy=\"most_frequent\")\n",
        "# fit and transform train data by replacing NaN with the most frequent value of the attribute\n",
        "i_train=pd.DataFrame(imp.fit_transform(train_pd))\n",
        "i_train.columns=train_pd.columns\n",
        "i_train.index=train_pd.index\n",
        "\n",
        "# transform the test data using the same model\n",
        "i_test = imp.transform(test_pd.values)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtvh5TWcEQnj",
        "outputId": "534225a4-72b8-4620-85f7-33a913e3f46f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(train_pd.shape)\n",
        "print(i_train.shape)\n",
        "\n",
        "print(test_pd.shape)\n",
        "print(i_test.shape)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(552, 15)\n",
            "(552, 15)\n",
            "(138, 15)\n",
            "(138, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuYB4UObEuGh"
      },
      "source": [
        "###Χειρισμός κατηγορικών χαρακτηριστικών\n",
        "\n",
        "Για τον χειρισμό των κατηγορικών χαρακτηριστικών θα μετατρέψουμε τα μη διατεταγμένα χαρακτηριστικά με m τιμές σε m binary χαρακτηριστικά από τα οποία μόνο ένα είναι ενεργό κάθε φορά. Τα μη διατεταγμένα χαρακτηριστικά είναι τα: \n",
        "    0,3,4,5,6,8,9,11,12\n",
        "Επομένως εκτελούμε:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg0Co3aQHerv"
      },
      "source": [
        "# οι κολόνες 0,3,4,5,6,8,9,11,12 έχουν κατηγορικές μεταβλητές. \n",
        "# Using \"get_dummies\" we convert to binary characteristics\n",
        "\n",
        "# converting train data\n",
        "dummies_train = pd.get_dummies(i_train, columns=[0,3,4,5,6,8,9,11,12])\n",
        "\n",
        "\n",
        "# converting test data\n",
        "# convert to dataframe \n",
        "mtdf = pd.DataFrame(i_test)\n",
        "dummies_test = pd.get_dummies(mtdf, columns=[0,3,4,5,6,8,9,11,12])\n",
        "\n"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtXtMrtVQuzN"
      },
      "source": [
        "Σε αυτό το σημείο θα πρέπει να έχουν δημιουργηθεί για κάθε τιμή των χαρακτηριστικών του συνόλου δεδομένων στο train dataset δυαδικά χαρακτηριστικά που δηλώνουν αν έχει ή όχι το δείγμα για το συγκεκριμένο χαρακτηριστικό αυτήν τιμή. Το ίδιο ισχύει και για το σύνολο δεδομένων στο test dataset. Ωστόσο, το δεύτερο είναι πολύ μικρότερο από το πρώτο και ανάλογα το πώς έγινε το split κάποιες τιμές για ορισμένα χαρακτηριστικά μπορεί να μην περιέχονται και στα 2 σύνολα, με αποτέλεσμα διαφορετικά σύνολα στηλών στα train και test. Για να αντιμετωπίσουμε αυτό το ζήτημα βρίσκουμε τις στήλες που εμφανίζονται μόνο στο ένα σύνολο δεδομένων και τις προσθέτουμε στο άλλο θέτοντας τιμή **0**. Αυτό το κάνουμε και για τα δύο σύνολα, αφού αν και λιγότερο πιθανό, μία τιμή ενός χαρακτηριστικού δύναται να περιέχεται στο test dataset και όχι στο train dataset. Ακολουθεί η υλοποίηση των παραπάνω:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1-Z7DCYIX8n",
        "outputId": "717bf2b3-7fa8-4f66-b91f-7cbf92b1a3cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "add_to_test = set(dummies_train.columns.values) - set(dummies_test.columns.values)\n",
        "for col in add_to_test:\n",
        "    print(col)\n",
        "    dummies_test[col] = 0\n",
        "dummies_test = dummies_test[dummies_train.columns]\n",
        "print(dummies_test.shape[1] == dummies_test.shape[1])\n",
        "\n",
        "# MAYBE DO IT FOR TRAIN TO"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6_dd\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT_QqvfRWIOe"
      },
      "source": [
        "Επομένως, με αυτόν τον τρόπο διασφαλίζουμε ότι τα δύο σύνολα έχουν <u>τα ίδια χαρακτηριστικά και με την ίδια σειρά</u>. \n",
        "\n",
        "Τέλος, μετατρέπουμε σε αριθμητικές τιμές:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTIxsYqhWaWu",
        "outputId": "3f208b19-8d54-44e1-bb30-5de055e0d363",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Μετατρέπουμε σε αριθμητικές τιμές (pd.to_numeric) και σε numpy array (.values)\n",
        "final_train = dummies_train.apply(pd.to_numeric).values\n",
        "print(final_train.shape)\n",
        "# Μετατρέπουμε σε αριθμητικές τιμές (pd.to_numeric) και σε numpy array (.values)\n",
        "final_test = dummies_test.apply(pd.to_numeric).values\n",
        "print(final_test.shape)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(552, 46)\n",
            "(138, 46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYmrUc66NnUJ"
      },
      "source": [
        "Βλέπουμε, όπως αναμενόταν, ότι το πλήθος των χαρακτηριστικών φαίνεται έχει πλέον αυξηθεί.\n",
        "Στην πραγματικότητα παραμένουν τα ίδια χαρακτηριστικά, ωστόσο για κάθε κατηγορικό χαρακτηριστικό Κ, δημιουργούμε το χαρακτηριστικό έχειΚ_V για κάθε δυνατή τιμή του K, V, το οποίο έχει τιμή 1 μόνο αν το συγκεκριμένο δείγμα έχει την τιμή V για το χαρακτηριστικό αυτό."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tNVM0Z73x5-"
      },
      "source": [
        "Εκτελούμε τις παρακάτω εντολές για να βρούμε τη διακύμανση στις τιμές των διαφόρων χαρακτηριστικών:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "YZUXF8pHpiHE",
        "outputId": "87a052ce-3e42-477b-9767-0c39f51a2bbb"
      },
      "source": [
        "\n",
        "train_variance = train.var(axis=0)\n",
        "print(train_variance)\n",
        "print(np.max(train_variance))\n",
        "print(train_variance.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-eb3aa863a9f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0masdasdasda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_variance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_variance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_variance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_variance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'asdasdasda' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmD18iivv6_8"
      },
      "source": [
        "from itertools import combinations\n",
        "\n",
        "def get_transformers(my_list):\n",
        "    \"\"\"\n",
        "    Function that gets a list of all the transformers that can be used\n",
        "    and returns a list of subists, where each sublist is a unique combination\n",
        "    of the transformers.\n",
        "    \"\"\"\n",
        "    sublists = []\n",
        "    for i in range(0, len(my_list)+1):\n",
        "        temp = [list(x) for x in combinations(my_list, i)]  #get the combinations with i elements\n",
        "        # scaler and min_max_scaler will not be used in the same pipeline. The same for ros and rus\n",
        "        if not (('scaler' in temp and 'min_max_scaler') or ('ros' in temp and 'rus' in temp)):\n",
        "            sublists.extend(temp)    #and add to initial list\n",
        "    return sublists\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5l4kqHEiVaf"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn import neighbors\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from imblearn.pipeline import Pipeline\n",
        "\n",
        "# import the packages for the transformers and classifiers we will use\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.preprocessing import StandardScaler # φέρνουμε τον StandarScaler ως transformer που έχει .transform kai ΄όχι ως scale()\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import preprocessing\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# create imputer that will replace NaN with the most frequent value\n",
        "imp=SimpleImputer(missing_values=np.NaN,strategy=\"most_frequent\")\n",
        "# fit and transform train data by replacing NaN with the most frequent value of the attribute\n",
        "i_train=train.DataFrame(imp.fit_transform(df))\n",
        "i_train.columns=df.columns\n",
        "i_train.index=df.index\n",
        "\n",
        "# transform the test data using the same model\n",
        "i_test = imp.transform(test_df.values)\n",
        "\n",
        "\n",
        "\n",
        "selector = VarianceThreshold()\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "scaler = StandardScaler()\n",
        "ros = RandomOverSampler()\n",
        "rus = RandomUnderSampler()\n",
        "pca = PCA()\n",
        "dummy = DummyClassifier()\n",
        "gnb = GaussianNB()\n",
        "knn = neighbors.KNeighborsClassifier(n_jobs=-1) # η παράμετρος n_jobs = 1 χρησιμοποιεί όλους τους πυρήνες του υπολογιστή\n",
        "\n",
        "\n",
        "vthresholds = [0, 1000, 10000, 1000000] #προσαρμόζουμε τις τιμές μας στο variance που παρατηρήσαμε\n",
        "scalers = [None, min_max_scaler, scaler]\n",
        "samplers = [None, ros, rus]\n",
        "num_n_components = [10, 20, 30, 40, 50, 60]\n",
        "\n",
        "k = [1, 6] # η υπερπαράμετρος του ταξινομητή\n",
        "scoring = ['f1_macro', 'f1_micro']\n",
        "\n",
        "\n",
        "# dict that maps the transformer with its model\n",
        "transformers = {'selector': selector,\n",
        "         'scaler': scaler,\n",
        "         'min_max_scaler': min_max_scaler,\n",
        "         'ros': ros,\n",
        "         'rus': rus,\n",
        "         'pca': pca\n",
        "        }\n",
        "\n",
        "# dict that maps the classifier with its model\n",
        "classifiers = {\n",
        "    'dummy': dummy,\n",
        "    'gnb': gnb,\n",
        "    'kNN': knn\n",
        "\n",
        "}\n",
        "\n",
        "# dict that maps the classifier with the appropriate syntax of the\n",
        "# gridsearchCV attribute\n",
        "est_mapper_keys = {\n",
        "    'selector': 'selector__threshold',\n",
        "    'pca': 'pca__n_components',\n",
        "    'dummy': 'dummy__strategy',\n",
        "    'kNN': 'kNN__n_neighbors'\n",
        "}\n",
        "\n",
        "# dict that maps the transformer/classifier with the values of its arguments\n",
        "est_values_mapper = {\n",
        "    'selector': [0, 1000, 10000, 1000000],\n",
        "    'pca': [10, 20, 30, 40, 50, 60],\n",
        "    'dummy': ['uniform', 'constant_0', 'constant_1', 'most_frequent', 'stratified'],\n",
        "    'kNN': [1, 6]\n",
        "}\n",
        "\n",
        "\n",
        "def getEstDict():\n",
        "    \"\"\"\n",
        "    Function that returns the dictionary that will be used as argument\n",
        "    for the GridSearchCV.\n",
        "    For every step in the steps of the pipeline it checks whether\n",
        "    there can be any argument for the GridSearchCV function and if there is, \n",
        "    it adds to the dictionary the proper attribute name and its value.\n",
        "    \"\"\"\n",
        "    dict = {}\n",
        "    for step in steps:\n",
        "        if step[0] in est_mapper_keys:\n",
        "            dict[est_mapper_keys[step[0]]] = est_values_mapper[step[0]]\n",
        "    return dict\n",
        "\n",
        "\n",
        "for sequence in get_transformers(transformers.keys()):\n",
        "    # add trasformers\n",
        "    steps = [(trans, transformers[trans]) for trans in sequence]\n",
        "    # add classifier\n",
        "    for classifier in classifiers.keys():\n",
        "        if len(steps): steps.pop()  #delete previous classifier\n",
        "        steps.append((classifier, classifiers[classifier])) #add new classifier\n",
        "        # create Pipe\n",
        "        pipe = Pipeline(steps=steps, memory = 'tmp')\n",
        "        estimator_dict = getEstDict()\n",
        "        # print(estimator_dict)\n",
        "        estimator = GridSearchCV(pipe, estimator_dict, cv=10, scoring='f1_macro', n_jobs=-1)\n",
        "        start_time = time.time()\n",
        "        estimator.fit(i_train, train_labels)\n",
        "        preds = estimator.predict(i_test)\n",
        "        print(\"\\n\\n\\n\",steps)\n",
        "        print(\"Συνολικός χρόνος fit και predict: %s seconds\" % (time.time() - start_time))\n",
        "        print(classification_report(test_labels, preds))\n",
        "        break\n",
        "    break\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ObxRFPVj1mb"
      },
      "source": [
        "estimator.get_params().keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkpJu5odkVO3"
      },
      "source": [
        "print(estimator.best_estimator_)\n",
        "print(estimator.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}