{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Lab_1.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgM-C62wlQTm"
      },
      "source": [
        "---\n",
        "#Άσκηση 1. Επιβλεπόμενη Μάθηση: Ταξινόμηση\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTjsQhI7orML"
      },
      "source": [
        "#Στοιχεία Ομάδας\n",
        "<u>Συνεργάτες</u>:\n",
        "\n",
        "Δούλης Κωνσταντίνος 03116175\n",
        "\n",
        "Καλογερόπουλος Ιωάννης 03116117\n",
        "\n",
        "Κατσίκας-Μουρούτσος Γεώργιος 03116132\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik7SPquAmwND"
      },
      "source": [
        "Αρχικά, ενημερώνουμε τις βιβλιοθήκες που θα χρησιμοποιηθούν (έχει προστεθεί στην αρχή του κελιού η magic command %%capture ώστε να μην εμφανίζονται στο stdouput οι πληροφορίες των εγκαταστάσεων, για να είναι πιο ευανάγνωστο):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7_jP0HklpR1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e0c85dae-1e4f-406f-f11c-057a3d00fd08"
      },
      "source": [
        "!pip install --upgrade pip #upgrade pip package installer\n",
        "!pip install scikit-learn --upgrade #upgrade scikit-learn package\n",
        "!pip install numpy --upgrade #upgrade numpy package\n",
        "!pip install pandas --upgrade #--upgrade #upgrade pandas package\n",
        "!pip install -U tensorflow\n",
        "!pip install --upgrade imbalanced-learn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pip in /usr/local/lib/python3.6/dist-packages (20.2.4)\n",
            "Requirement already up-to-date: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.23.2)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.17.0)\n",
            "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (2.1.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.5)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.19.4-cp36-cp36m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5 MB 99 kB/s \n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
            "\n",
            "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
            "\n",
            "tensorflow 2.3.0 requires numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.4 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.19.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pandas in /usr/local/lib/python3.6/dist-packages (1.1.4)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.19.4)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.3.1-cp36-cp36m-manylinux2010_x86_64.whl (320.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4 MB 21 kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Collecting numpy<1.19.0,>=1.16.0\n",
            "  Downloading numpy-1.18.5-cp36-cp36m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 1.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.33.2)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.35.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow) (50.3.2)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.2)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3.3)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.11.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
            "Installing collected packages: numpy, tensorflow\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.4\n",
            "    Uninstalling numpy-1.19.4:\n",
            "      Successfully uninstalled numpy-1.19.4\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUYHWBBvjb9w"
      },
      "source": [
        "import warnings \n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9L1ZXDKncGu"
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "def download(url, file):\n",
        "    if not os.path.isfile(file):\n",
        "        urlretrieve(url,file)\n",
        "        print(\"File downloaded\")\n",
        "\n",
        "download('http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data', 'crx.data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuojFZSVqK3y"
      },
      "source": [
        "#Mικρό Dataset: Japanese Credit Screening\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q2QlhnxrYXY"
      },
      "source": [
        "##Πληροφορίες dataset\n",
        "\n",
        "Το dataset που εξετάζουμε παρέχει πληροφορίες σχετικά με άτομα στα οποία χορηγήθηκε ή όχι πίστωση από Ιαπωνική εταιρεία. Τα δεδομένα παράχθηκαν ύστερα από σχετικές ερωτήσεις στους πελάτες τις εταιρείας."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAMbop8Pr0B8"
      },
      "source": [
        "##Περιγραφή χαρακτηριστικών του dataset\n",
        "\n",
        "To dataset διαθέτει <b>690 δείγματα</b>, με το καθένα από αυτά να διαθέτει <b>15 χαρακτηριστικά συν 1 που είναι η κλάση</b>.\n",
        "\n",
        "Το κάθε δείγμα αντιστοιχεί σε μία γραμμή του αρχείου, ενώ η κάθε στήλη σε ένα χαρακτηριστικό του. <b>Η τελευταία στήλη δηλώνει την κλάση που ανήκει το δείγμα</b>. Συγκεκριμένα, το σύμβολο <b>\"+\"</b> δηλώνει ότι η πίστωση <b>εγκρίθηκε</b>, ενώ το σύμβολο <b>\"-\"</b> δηλώνει ότι <b>δεν εγκρίθηκε</b>.\n",
        "Επομένως, η δομή του αρχείου είναι κατάλληλη για να ξεκινήσουμε την προεπεξεργασία του dataset.\n",
        "\n",
        "Για λόγους απορήτου τα attribute names και values έχουν αντικατασταθεί με τυχαία σύμβολα, που δεν παρέχουν κάποια σχετική πληροφορία σχετικά με το τι περιγράφουν.\n",
        "\n",
        "Το είδος των χαρακτηριστικών φαίνεται παρακάτω:\n",
        "\n",
        "| Attribute | Type of Data                                 | Description    |\n",
        "|-----------|----------------------------------------------|----------------|\n",
        "| A1        | b, a.                                        | Male           |\n",
        "| A2        | continuous.                                  | Age            |\n",
        "| A3        | continuous.                                  | Debt           |\n",
        "| A4        | u, y, l, t.                                  | Married        |\n",
        "| A5        | g, p, gg.                                    | BankCustomer   |\n",
        "| A6        | c, d, cc, i, j, k, m, r, q, w, x, e, aa, ff. | EducationLevel |\n",
        "| A7        | v, h, bb, j, n, z, dd, ff, o.                | Ethnicity      |\n",
        "| A8        | continuous.                                  | YearsEmployed  |\n",
        "| A9        | t, f.                                        | PriorDefault   |\n",
        "| A10       | t, f.                                        | Employed       |\n",
        "| A11       | continuous.                                  | CreditScore    |\n",
        "| A12       | t, f.                                        | DriversLicense |\n",
        "| A13       | g, p, s.                                     | Citizen        |\n",
        "| A14       | continuous.                                  | ZipCode        |\n",
        "| A15       | continuous.                                  | Income         |\n",
        "| A16       | +,-                                          | Approved (CLASS ATTRIBUTE)       |\n",
        "\n",
        "\n",
        "\n",
        "* Τα χαρακτηριστικά που έχουν continuous τιμές αποτελουν <b>διατεταγμένα </b>χαρακτηριστικά, ενώ αυτά που έχουν διακριτές τιμές, δηλαδή σύμβολα, <b>μη διατεταγμένα</b>.\n",
        "\n",
        "* Στο dataset <b>δεν</b> υπάρχουν επικεφαλίδες, καθώς ούτε και αρίθμηση γραμμών.\n",
        "\n",
        "* Οι ετικέτες και οι σημασίες τους είναι:\n",
        "    * \"+\": Εγκρίθηκε η πίστωση\n",
        "    * \"-\": Απορρίφθηκε η πίστωση\n",
        "\n",
        "    Οι ετικέτες αυτές βρίσκονται στην τελευταία στήλη, ενώ οι τιμές τους θα πρέπει να μετατραπούν όπως φαίνεται παρακάτω:\n",
        "    * \"+\" -> 1\n",
        "    * \"-\" -> 0\n",
        "\n",
        "* <b>Υπάρχουν απουσιάζουσες τιμές</b>. Συγκεκριμένα, 37 δείγματα (5%) του συνολικού αριθμού δειγμάτων παρουσιάζουν μία ή παραπάνω απώλειες δεδομένων. \n",
        "\n",
        "Όλα τα παραπάνω φαίνονται παρακάτω:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW4faeHk8TKM"
      },
      "source": [
        "##Προεπεξεργασία των δεδομένων\n",
        "Aρχικά διαβάζουμε το αρχείο <b>crx.data</b>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNOetXVa77M7"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"crx.data\", header=None)\n",
        "# print the five first samples\n",
        "print(df[:5])\n",
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdZtApI_9Fab"
      },
      "source": [
        "###Αντιστοίχιση κλάσεων\n",
        "Παρατηρούμε, όπως ειπώθηκε και παραπάνω, ότι οι κλάσεις δηλώνονται στην τελευταία στήλη με τα σύμβολα +/-. \n",
        "\n",
        "Μετατρέπουμε τα \"+\" σε 1 και τα \"-\" σε 0:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3M1H7ST9O9x"
      },
      "source": [
        "# create mapper for each symbol\n",
        "class_mapper = {\"+\": 1, \"-\": 0}\n",
        "# since these symbols occur only on the last column we dont specify the column\n",
        "df = df.replace(class_mapper)\n",
        "print(df[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p35rb4qAuRYR"
      },
      "source": [
        "###Εντοπισμός απουσιάζουσων τιμών χαρακτηριστικών\n",
        "\n",
        "Στη συνέχεια, θα εντοπίσουμε τις απουσιάζουσες τιμές. Αυτές δηλώνονται στο dataset με τον χαρακτήρα \"?\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4snaGwf-KV3"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# replace \"?\" with np.NaN\n",
        "df.replace('?',np.NaN,inplace=True)\n",
        "\n",
        "# calculate the number of samples with at least one missing attribute\n",
        "# df.isna() is the mask of df where each element is True if is NaN\n",
        "# s is the length of the list of samples that have at least one True(NaN)\n",
        "num_of_incomplete_samples = len([i for i in np.array(df.isna()) if True in i])\n",
        "\n",
        "print(\"The samples of the dataset that have at least one missing attribute are \", num_of_incomplete_samples)\n",
        "print(\"Which means that the \", num_of_incomplete_samples*100/df.shape[0], \"% of the samples have missing values.\", sep=\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdHVwDC30eRv"
      },
      "source": [
        "Από τα παραπάνω βλέπουμε ότι αν θέλαμε να αγνοήσουμε τα δείγματα που έχουν missing values θα έπρεπε να \"πετάξουμε\" το 5% των δειγμάτων, αριθμός που <b>θα επηρέαζε αρνητικά τα αποτελέσματα</b>.\n",
        "\n",
        "Προκειμένου να εξετάσουμε αν οι περισσότερες τιμές που λείπουν προέρχονται απο συγκεκριμένα (λίγα) χαρακτηριστικά (και αν αυτά έχουν ίσως και μηδενική διακύμανση, δηλαδή  χαρακτηριστικά με σταθερές τιμές), οπότε θα μπορούσαμε να τα αγνοήσουμε, εκτελούμε:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkPRNBdi32T1"
      },
      "source": [
        "# the list of the number of missing values for each attribute occurs by \n",
        "# summing the elements of the inverse of df, without df's last column.\n",
        "# df's last column is the class attribute and it is alwasy present.\n",
        "incomplete_attrs = [sum(i) for i in np.array(df.isna())[:,:df.shape[1]-1].T]\n",
        "print(\"For each attribute of the dataset, the number of the missing values is\")\n",
        "print(incomplete_attrs)\n",
        "# print(sum(incomplete_attrs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fXm2-b_32zj"
      },
      "source": [
        "Βλέπουμε, επομένως, ότι από τα 690 δείγματα το πολύ σε 13 από αυτά ένα χαρακτηριστικό δεν έχει τιμές. Επομένως, δεν έχει νόημα να αγνοήσουμε το χαρακτηριστικό αυτό, ακόμα και αν είναι αυτό με τις περισσότερες ελλείψεις, αφού αυτές είναι πολύ λίγες και θα χάναμε πολύ πληροφορία.\n",
        "\n",
        "Τελικά η πιο συμφέρουσα λύση είναι να αντικαταστήσουμε τα NaN με τιμές που προκύπτουν με βάση τις τιμές των χαρακτηριστικών σε προηγούμενα δείγματα. <u>Συγκεκριμένα, αντικαθιστούμε τα NaN με την τιμή του χαρακτηριστικού που εμφανίζεται πιο συχνά</u>.\n",
        "\n",
        "Την αντικατάσταση των απουσιάζουσων τιμών θα τη χειρίζεται το πρώτο κομμάτι προεπεξεργασίας του Pipeline, που θα οριστεί στη συνέχεια και θα πρόκειται για έναν μετασχηματιστή Imputer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iIfeNp-F20h"
      },
      "source": [
        "Οι κλάσεις βρίσκονται στην τελευταία στήλη του συνόλου δεδομένων. Οπότε για να βρούμε το πλήθος τους και τα ποσοστά δειγμάτων τους επί του συνόλου:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc32MgrVGpPE"
      },
      "source": [
        "num_of_rows = df.shape[0]\n",
        "num_of_attrs = df.shape[1] - 1 #remove one element because of the class attribute\n",
        "\n",
        "# get labesl and features\n",
        "labels_df = df.iloc[:, [num_of_attrs]] # τα labels είναι στην τελευταία κολώνα\n",
        "features_df = df.iloc[:, 0:num_of_attrs]  # τα features είναι όλες οι προηγούμενες κολώνες\n",
        "\n",
        "labels = labels_df.values.reshape(num_of_rows,)\n",
        "features = features_df.values\n",
        "# convert to int\n",
        "labels.astype(int)\n",
        "labels = np.array(labels, dtype='int64')\n",
        "# print(labels.shape)\n",
        "\n",
        "# find how many of each class\n",
        "bin_count = np.bincount(labels)\n",
        "print (\"frequencies:\", bin_count)\n",
        "print(\"The percentage of 0's in data: \", bin_count[0]*100/sum(bin_count), \"%.\")\n",
        "print(\"The percentage of 1's in data: \", bin_count[1]*100/sum(bin_count), \"%.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW0hDxy-LS28"
      },
      "source": [
        "Καταλήγουμε, λοιπόν, ότι τα ποσοστά είναι περίπου 55.5% για την κλάση 0 και 44.5% για την κλάση 1. Επομένως, το dataset είναι <b>ισορροπημένο</b>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqMTLEANH4ST"
      },
      "source": [
        "# print(labels[:5])\n",
        "print(features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe7knthjExus"
      },
      "source": [
        "###Εντοπισμός κατηγορικών χαρακτηριστικών\n",
        "Το dataset περιλαμβάνει εκτός από χαρακτηριστικά με συνεχείς αριθμητικές τιμές και κατηγορικά χαρακτηριστικά. Από τις πληροφορίες που έχουμε για το dataset, τα χαρακτηριστικά που είναι κατηγορικά είναι αυτά που αντιστοιχούν στις στήλες:\n",
        "\n",
        "    0,3,4,5,6,8,9,11,12\n",
        "\n",
        "Ο χειρισμός των κατηγορικών συμβόλων θα γίνει μετά το χειρισμό των απουσιάζουσων τιμών και πριν το Cross Validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yni1P1F6D-89"
      },
      "source": [
        "##Διαχωρισμός του dataset\n",
        "Διαχωρίζουμε το dataset σε train και test set, χρησιμοποιώντας το 80% των δεδομένων για το training και το 20% για το testing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hze8obkLEg0K"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "train, test, train_labels, test_labels = train_test_split(features, labels, test_size=0.2, random_state=78)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOLwbo9dBD4a"
      },
      "source": [
        "train_pd = pd.DataFrame(data=train[:,:],    # values\n",
        "                 index=train[:,0])    # 1st column as index\n",
        "                  \n",
        "test_pd = pd.DataFrame(data=test[:,:],    # values\n",
        "                 index=test[:,0])    # 1st column as index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrNfyxLaBd8Y"
      },
      "source": [
        "print(train.shape)\n",
        "print(train_pd.shape)\n",
        "\n",
        "print(test.shape)\n",
        "print(test_pd.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo0VGdmqlTjd"
      },
      "source": [
        "from collections import defaultdict\n",
        "# just for checking...\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "check = defaultdict(int)\n",
        "for i in range(0,len(train_labels)):\n",
        "    check['pos_train'] += train_labels[i] == 1\n",
        "    check['neg_train'] += train_labels[i] == 0\n",
        "    \n",
        "\n",
        "# for i in range(0,len(test_labels)):\n",
        "#     check['pos_test'] += test_labels[i] == 1\n",
        "#     check['neg_test'] += test_labels[i] == 0\n",
        "    \n",
        "# print(check)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6hs-A0Q5a1j"
      },
      "source": [
        "##TO BE DELETED"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5QAIBec92x_"
      },
      "source": [
        "###Variance Threshold\n",
        "\n",
        "Παρακάτω υλοποιείται η συνάρτηση που αρχικοποιεί selector Variance Threshold, με κατόφλι που δίνεται από το όρισμα threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jV9JVV6X68pn"
      },
      "source": [
        "# from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "# def Variance_Threshold(data, threshold=0):\n",
        "#     # αρχικοποιούμε έναν selector\n",
        "#     train, test, train_labels, test_labels = data\n",
        "#     selector = VarianceThreshold(threshold = threshold)\n",
        "#     train_reduced = selector.fit_transform(train)\n",
        "#     test_reduced = selector.transform(test)\n",
        "\n",
        "#     return (train_reduced, test_reduced, train_labels, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug166UU_88B1"
      },
      "source": [
        "###PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHoEFrk689m2"
      },
      "source": [
        "# from sklearn.decomposition import PCA\n",
        "\n",
        "# def PCA_selector(data, n):\n",
        "#     \"\"\"\n",
        "#     Function that creates a PCA selector that learns the train data\n",
        "#     and transforms train and test data.\n",
        "#     \"\"\"\n",
        "#     train, test, train_labels, test_labels = data\n",
        "#     # αρχικοποιούμε τον selector\n",
        "#     pca = PCA(n_components=n)\n",
        "#     trainPCA = pca.fit_transform(train)\n",
        "#     testPCA = pca.transform(test)\n",
        "\n",
        "#     return (trainPCA, testPCA, train_labels, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2gtTLCt9zko"
      },
      "source": [
        "###Κανονικοποίηση"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIz6YOlw-KEu"
      },
      "source": [
        "# from scipy import stats as st\n",
        "\n",
        "# def StandarizerTransformer(data, method):\n",
        "#     \"\"\"\n",
        "#     Function that standarizes the data using two different methods.\n",
        "#     minmax and zscore.    \n",
        "#     \"\"\"\n",
        "#     train, test, train_labels, test_labels = data\n",
        "#     if method==\"zscore\":\n",
        "#         std_train = st.zscore(train)\n",
        "#     elif method==\"minmax\":\n",
        "#         std_train = (train - np.min(train) )/ (np.max(train) - np.min(train))\n",
        "#     else:\n",
        "#         std_train = train\n",
        "#     return (std_train, test, train_labels, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1qXIMpk_x--"
      },
      "source": [
        "###Εξισορρόπηση dataset - Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QaC5aRuVZlL"
      },
      "source": [
        "# !pip install six"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLjaejVoANn9"
      },
      "source": [
        "# # from sklearn.externals import six\n",
        "# # from imblearn.over_sampling import RandomOverSampler\n",
        "# # from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# def sampling(data, method):\n",
        "#     train, test, train_labels, test_labels = data\n",
        "#     if method == \"over\":\n",
        "#         ros = RandomOverSampler()\n",
        "#         # mlb = MultiLabelBinarizer().fit(['0', '1', '2', '3', '4'])\n",
        "#         # tmplabels = np.asarray(mlb.inverse_transform(C_trainDataTargets)).flatten()\n",
        "#         train_resampled, train_labels_resampled = ros.fit_sample(train,train_labels)\n",
        "#         trainTargets_resampled = mlb.transform(trainTargets_resampled)\n",
        "#     elif method == 'under':\n",
        "#         ros = RandomUnderSampler()\n",
        "#         # mlb = MultiLabelBinarizer().fit(['0', '1', '2', '3', '4'])\n",
        "#         # tmplabels = np.asarray(mlb.inverse_transform(C_trainDataTargets)).flatten()\n",
        "#         train_resampled, train_labels_resampled = ros.fit_sample(train,train_labels)\n",
        "#         trainTargets_resampled = mlb.transform(trainTargets_resampled)\n",
        "    \n",
        "#     return (train_resampled, test, train_labels_resampled, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIZND57tVwK0"
      },
      "source": [
        "# from sklearn.dummy import DummyClassifier\n",
        "# from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n",
        "# def createClassifier(classifier, argument=5):\n",
        "\n",
        "#     if classifier == 'dummy':\n",
        "#         clf = DummyClassifier(strategy=argument)\n",
        "#     elif classifier == 'gnb':\n",
        "#         clf = GaussianNB()\n",
        "#     elif classifier == 'knn':\n",
        "#         clf = KNeighborsClassifier(n_neighbors=argument)\n",
        "    \n",
        "#     return clf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCPJcKB9ZJm0"
      },
      "source": [
        "# def Pipeline (data, steps):\n",
        "#     \"\"\"\n",
        "#     Args:\n",
        "#         * data: (train, train_labels, test, test_labels)\n",
        "#         * steps: list of tuples, where each tuple's 1st el \n",
        "#         is the transformer/classifier and the 2nd el\n",
        "#         is the arguments of the function for the\n",
        "#         transformer/classifier\n",
        "#     \"\"\"\n",
        "\n",
        "#     mapper = {\n",
        "#         'selector': Variance_Threshold,\n",
        "#         'scaler': StandarizerTransformer,\n",
        "#         'sampler': sampling,\n",
        "#         'pca': PCA_selector,\n",
        "#         'kNN': createClassifier,\n",
        "#         'dummy': createClassifier\n",
        "#     }\n",
        "\n",
        "#     classifiers = set(['kNN', 'uniform', 'constant_0', 'constant_1', 'most_frequent', 'stratified'])\n",
        "\n",
        "#     for step in steps:\n",
        "#         if step[0] not in classifiers:\n",
        "#             # update data\n",
        "#             data = mapper[step[0]](step[1])\n",
        "\n",
        "#         else:\n",
        "#             # return model\n",
        "#             return (data, createClassifier(step[0], step[1]))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Myc4SzW_5CKC"
      },
      "source": [
        "##Κατασκευή Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNwAJkIN_XGK"
      },
      "source": [
        "###Χειρισμός απουσιάζουσων τιμών\n",
        "Όπως καταλήξαμε θα χρησιμοποιηθεί ο μετασχηματιστής Imputer, για τον χειρισμό των απουσιάζουσων τιμών, θέτοντας σε αυτές την πιο συχνή τιμή του χαρακτηριστικού"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-EJG3JR_wjD"
      },
      "source": [
        "# create imputer that will replace NaN with the most frequent value\n",
        "imp=SimpleImputer(missing_values=np.NaN,strategy=\"most_frequent\")\n",
        "# fit and transform train data by replacing NaN with the most frequent value of the attribute\n",
        "i_train=pd.DataFrame(imp.fit_transform(train_pd))\n",
        "i_train.columns=train_pd.columns\n",
        "i_train.index=train_pd.index\n",
        "\n",
        "# transform the test data using the same model\n",
        "i_test = imp.transform(test_pd.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtvh5TWcEQnj"
      },
      "source": [
        "print(train_pd.shape)\n",
        "print(i_train.shape)\n",
        "\n",
        "print(test_pd.shape)\n",
        "print(i_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuYB4UObEuGh"
      },
      "source": [
        "###Χειρισμός κατηγορικών χαρακτηριστικών\n",
        "\n",
        "Για τον χειρισμό των κατηγορικών χαρακτηριστικών θα μετατρέψουμε τα μη διατεταγμένα χαρακτηριστικά με m τιμές σε m binary χαρακτηριστικά από τα οποία μόνο ένα είναι ενεργό κάθε φορά. Τα μη διατεταγμένα χαρακτηριστικά είναι τα: \n",
        "    0,3,4,5,6,8,9,11,12\n",
        "Επομένως εκτελούμε:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg0Co3aQHerv"
      },
      "source": [
        "# οι κολόνες 0,3,4,5,6,8,9,11,12 έχουν κατηγορικές μεταβλητές. \n",
        "# Using \"get_dummies\" we convert to binary characteristics\n",
        "\n",
        "# converting train data\n",
        "dummies_train = pd.get_dummies(i_train, columns=[0,3,4,5,6,8,9,11,12])\n",
        "\n",
        "\n",
        "# converting test data\n",
        "# convert to dataframe \n",
        "mtdf = pd.DataFrame(i_test)\n",
        "dummies_test = pd.get_dummies(mtdf, columns=[0,3,4,5,6,8,9,11,12])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtXtMrtVQuzN"
      },
      "source": [
        "Σε αυτό το σημείο θα πρέπει να έχουν δημιουργηθεί για κάθε τιμή των χαρακτηριστικών του συνόλου δεδομένων στο train dataset δυαδικά χαρακτηριστικά που δηλώνουν αν έχει ή όχι το δείγμα για το συγκεκριμένο χαρακτηριστικό αυτήν τιμή. Το ίδιο ισχύει και για το σύνολο δεδομένων στο test dataset. Ωστόσο, το δεύτερο είναι πολύ μικρότερο από το πρώτο και ανάλογα το πώς έγινε το split κάποιες τιμές για ορισμένα χαρακτηριστικά μπορεί να μην περιέχονται και στα 2 σύνολα, με αποτέλεσμα διαφορετικά σύνολα στηλών στα train και test. Για να αντιμετωπίσουμε αυτό το ζήτημα βρίσκουμε τις στήλες που εμφανίζονται μόνο στο ένα σύνολο δεδομένων και τις προσθέτουμε στο άλλο θέτοντας τιμή **0**. Αυτό το κάνουμε και για τα δύο σύνολα, αφού αν και λιγότερο πιθανό, μία τιμή ενός χαρακτηριστικού δύναται να περιέχεται στο test dataset και όχι στο train dataset. Ακολουθεί η υλοποίηση των παραπάνω:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1-Z7DCYIX8n"
      },
      "source": [
        "add_to_test = set(dummies_train.columns.values) - set(dummies_test.columns.values)\n",
        "for col in add_to_test:\n",
        "    dummies_test[col] = 0\n",
        "dummies_test = dummies_test[dummies_train.columns]\n",
        "print(dummies_test.shape[1] == dummies_test.shape[1])\n",
        "\n",
        "# MAYBE DO IT FOR TRAIN TO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT_QqvfRWIOe"
      },
      "source": [
        "Επομένως, με αυτόν τον τρόπο διασφαλίζουμε ότι τα δύο σύνολα έχουν <u>τα ίδια χαρακτηριστικά και με την ίδια σειρά</u>. \n",
        "\n",
        "Τέλος, μετατρέπουμε σε αριθμητικές τιμές:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTIxsYqhWaWu"
      },
      "source": [
        "# Μετατρέπουμε σε αριθμητικές τιμές (pd.to_numeric) και σε numpy array (.values)\n",
        "final_train = dummies_train.apply(pd.to_numeric).values\n",
        "print(final_train.shape)\n",
        "# Μετατρέπουμε σε αριθμητικές τιμές (pd.to_numeric) και σε numpy array (.values)\n",
        "final_test = dummies_test.apply(pd.to_numeric).values\n",
        "print(final_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYmrUc66NnUJ"
      },
      "source": [
        "Βλέπουμε, όπως αναμενόταν, ότι το πλήθος των χαρακτηριστικών φαίνεται έχει πλέον αυξηθεί.\n",
        "Στην πραγματικότητα παραμένουν τα ίδια χαρακτηριστικά, ωστόσο για κάθε κατηγορικό χαρακτηριστικό Κ, δημιουργούμε το χαρακτηριστικό έχειΚ_V για κάθε δυνατή τιμή του K, V, το οποίο έχει τιμή 1 μόνο αν το συγκεκριμένο δείγμα έχει την τιμή V για το χαρακτηριστικό αυτό."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmD18iivv6_8"
      },
      "source": [
        "from itertools import combinations\n",
        "\n",
        "def get_transformers(my_list):\n",
        "    \"\"\"\n",
        "    Function that gets a list of all the transformers that can be used\n",
        "    and returns a list of subists, where each sublist is a unique combination\n",
        "    of the transformers.\n",
        "    \"\"\"\n",
        "    sublists = []\n",
        "    for i in range(0, len(my_list)+1):\n",
        "        temp = [list(x) for x in combinations(my_list, i) if not (('scaler' in x and 'min_max_scaler' in x) or ('ros' in x and 'rus' in x))]  #get the combinations with i elements\n",
        "        # scaler and min_max_scaler will not be used in the same pipeline. The same for ros and rus\n",
        "        sublists.extend(temp)    #and add to initial list\n",
        "    return sublists\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5l4kqHEiVaf"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn import neighbors\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from imblearn.pipeline import Pipeline\n",
        "\n",
        "# import the packages for the transformers and classifiers we will use\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.preprocessing import StandardScaler # φέρνουμε τον StandarScaler ως transformer που έχει .transform kai ΄όχι ως scale()\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import preprocessing\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "from sklearn.metrics import plot_confusion_matrix, accuracy_score, f1_score\n",
        "\n",
        "\n",
        "\n",
        "selector = VarianceThreshold()\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "scaler = StandardScaler()\n",
        "ros = RandomOverSampler()\n",
        "rus = RandomUnderSampler()\n",
        "pca = PCA()\n",
        "dummy = DummyClassifier()\n",
        "gnb = GaussianNB()\n",
        "knn = neighbors.KNeighborsClassifier(n_jobs=-1) # η παράμετρος n_jobs = 1 χρησιμοποιεί όλους τους πυρήνες του υπολογιστή\n",
        "\n",
        "\n",
        "# vthresholds = [0, 1000, 10000, 1000000] #προσαρμόζουμε τις τιμές μας στο variance που παρατηρήσαμε\n",
        "# scalers = [None, min_max_scaler, scaler]\n",
        "# samplers = [None, ros, rus]\n",
        "# num_n_components = [10, 20, 30, 40, 50, 60]\n",
        "\n",
        "# k = [1, 6] # η υπερπαράμετρος του ταξινομητή\n",
        "scoring = ['f1_macro', 'f1_micro']\n",
        "\n",
        "\n",
        "# dict that maps the transformer with its model\n",
        "transformers = {'selector': selector,\n",
        "         'scaler': scaler,\n",
        "         'min_max_scaler': min_max_scaler,\n",
        "         'ros': ros,\n",
        "         'rus': rus,\n",
        "         'pca': pca\n",
        "        }\n",
        "\n",
        "# dict that maps the classifier with its model\n",
        "classifiers = {\n",
        "    'dummy': dummy,\n",
        "    'gnb': gnb,\n",
        "    'kNN': knn\n",
        "\n",
        "}\n",
        "\n",
        "# dict that maps the classifier with the appropriate syntax of the\n",
        "# gridsearchCV attribute\n",
        "est_mapper_keys = {\n",
        "    'selector': 'selector__threshold',\n",
        "    'pca': 'pca__n_components',\n",
        "    'dummy': 'dummy__strategy',\n",
        "    'kNN': 'kNN__n_neighbors'\n",
        "}\n",
        "\n",
        "# dict that maps the transformer/classifier with the values of its arguments\n",
        "est_values_mapper = {\n",
        "    'selector': [0, 1000, 10000, 1000000],\n",
        "    'pca': [10, 20, 30, 40, 50, 60],\n",
        "    'dummy': ['uniform', 'constant_0', 'constant_1', 'most_frequent', 'stratified'],\n",
        "    'kNN': [2, 5, 7, 9, 11]\n",
        "}\n",
        "\n",
        "\n",
        "def getEstDict(steps):\n",
        "    \"\"\"\n",
        "    Function that returns the dictionary that will be used as argument\n",
        "    for the GridSearchCV.\n",
        "    For every step in the steps of the pipeline it checks whether\n",
        "    there can be any argument for the GridSearchCV function and if there is, \n",
        "    it adds to the dictionary the proper attribute name and its value.\n",
        "    \"\"\"\n",
        "    dict = {}\n",
        "    for step in steps:\n",
        "        if step[0] in est_mapper_keys:\n",
        "            dict[est_mapper_keys[step[0]]] = est_values_mapper[step[0]]\n",
        "    return dict\n",
        "\n",
        "\n",
        "def runEstimators(my_transformers = transformers.keys(), my_classifiers =classifiers.keys() , cv=10, scoring='f1_macro', showResults = False,\n",
        "                  train, test, train_labels, test_labels):\n",
        "    f1_scores = defaultdict(str)\n",
        "    f1_scores['micro'] = {}\n",
        "    f1_scores['macro'] = {}\n",
        "    f1_scores['best_estimator'] = {}\n",
        "    f1_scores['fit_time'] = {}\n",
        "    f1_scores['predict_time'] = {}\n",
        "    \n",
        "    counter = 0\n",
        "    for sequence in get_transformers(my_transformers):\n",
        "        \n",
        "        # add trasformers\n",
        "        steps = []  #initialize\n",
        "        steps = [(trans, transformers[trans]) for trans in sequence]\n",
        "        \n",
        "        # add classifier\n",
        "        for classifier in my_classifiers:\n",
        "            counter+=1\n",
        "            # print(classifier)\n",
        "            if len(steps): steps.pop()  #delete previous classifier\n",
        "            steps.append((classifier, my_classifiers[classifier])) #add new classifier\n",
        "            \n",
        "            # create Pipe\n",
        "            pipe = Pipeline(steps=steps, memory = 'tmp')\n",
        "            if cv:\n",
        "                # create Estimator\n",
        "                estimator_dict = getEstDict(steps)\n",
        "                estimator = GridSearchCV(pipe, estimator_dict, cv=2, scoring=scoring, n_jobs=-1)\n",
        "                fit_start_time = time.time()    #start counting fit time\n",
        "                # fit estimator\n",
        "                estimator.fit(train, train_labels)\n",
        "                f1_scores['fit_time'][classifier+str(counter)] = time.time() - fit_start_time\n",
        "            else:\n",
        "                estimator = pipe.fit(train, train_labels)\n",
        "            \n",
        "            # test on test data\n",
        "            predict_start_time = time.time()    #start counting predict time\n",
        "            preds = estimator.predict(test)\n",
        "            # print(\"Total time fit and predict: %s seconds\" % (time.time() - start_time))\n",
        "            f1_scores['predict_time'][classifier+str(counter)] = time.time() - predict_start_time\n",
        "            f1_scores['micro'][classifier+str(counter)] = f1_score(test_labels, preds, average='micro')\n",
        "            f1_scores['macro'][classifier+str(counter)] = f1_score(test_labels, preds, average='macro')\n",
        "            if cv:\n",
        "                f1_scores['best_estimator'][classifier+str(counter)] =  estimator.best_estimator_\n",
        "                # print(estimator.best_params_)\n",
        "            if showResults:\n",
        "                print(\"The average f1-micro average of the \"+ classifier +\" classifier is: \", f1_score(test_labels, preds, average='micro'))\n",
        "                print(\"The average f1-macro average of the \"+ classifier +\" classifier is: \", f1_score(test_labels, preds, average='macro'))\n",
        "                print(\"The classification report:\")\n",
        "                print(classification_report(test_labels, preds, target_names=[\"rejected\", \"granted\"]))\n",
        "                disp1 = plot_confusion_matrix(estimator, test, test_labels,\n",
        "                                    display_labels=[\"rejected\", \"granted\"],\n",
        "                                    cmap=plt.cm.Blues)\n",
        "                plt.show()\n",
        "                print(\"================================================================================\")\n",
        "        #     break\n",
        "        # break\n",
        "    print(counter)\n",
        "    return f1_scores\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ObxRFPVj1mb"
      },
      "source": [
        "# estimator.get_params().keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkpJu5odkVO3"
      },
      "source": [
        "# print(estimator.best_estimator_)\n",
        "# print(estimator.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5mNVBXGdNRE"
      },
      "source": [
        "##Baseline Classification - Ταξινόμηση χωρίς προεπεξεργασία"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQB2ODT4dUbU"
      },
      "source": [
        "Έχοντας υλοποιήσει τα παραπάνω, θα εκπαιδευτούν στο train dataset οι διάφοροι ταξινομητές (Dummy, Gausian Naive Bayes και K-Nearest Neighbors), χωρίς κάποια προεπεξεργασία των δεδομένων, εκτός από τον χειρισμό των απουσιάζουσων τιμών και των κατηγορικών χαρακτηριστικών και με τις default παραμέτρους τους:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtOKCdXtcTcR"
      },
      "source": [
        "a = {1:11, 2:22}\n",
        "print(a.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7592my4QlkWK"
      },
      "source": [
        "# dict that maps the classifier with its model\n",
        "classifiers = {\n",
        "    'dummy': dummy,\n",
        "    'gnb': gnb,\n",
        "    'kNN': knn\n",
        "\n",
        "}\n",
        "\n",
        "f1_scores_default = runEstimators(my_transformers={}, my_classifiers=classifiers, cv=None, showResults=True, train=final_train, test=final_test, train_labels=train_labels, test_labels=test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeE5j1aUNA0P"
      },
      "source": [
        "Στη συνέχεια, έχοντας αποθηκεύσει τις τιμές των μετρικών F1-micro average και F1-macro average, τις αναπαριστούμε γραφικά:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3lXy5jkqduh"
      },
      "source": [
        "# plot for f1-macro average\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "clfs = f1_scores_default['macro'].keys()\n",
        "f1_macro_scores = f1_scores_default['macro'].values()\n",
        "ax.bar(clfs,f1_macro_scores, color='cyan', width=0.3)\n",
        "plt.title(\"F1 Macro Average\")\n",
        "plt.xlabel(\"Classifiers\")\n",
        "plt.ylabel(\"F1 Macro Score\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# plot for f1-micro average\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "clfs = f1_scores_default['micro'].keys()\n",
        "f1_macro_scores = f1_scores_default['micro'].values()\n",
        "ax.bar(clfs,f1_macro_scores, color='b', width=0.3)\n",
        "plt.title(\"F1 Micro Average\")\n",
        "plt.xlabel(\"Classifiers\")\n",
        "plt.ylabel(\"F1 Micro Score\")\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtOHr7PXNPxv"
      },
      "source": [
        "###Σχολιασμός αποτελεσμάτων\n",
        "\n",
        "Για την απόδοση των τριών ταξινομητών στα δεδομένα εισόδου έχουμε στη διάθεσή μας τις μετρικές:\n",
        "\n",
        "* **Precision**: Δηλώνει για κάθε κλάση την ακρίβεια των positive προβλέψεων, δηλαδή κατά πόσο ήταν θετικά αυτά που το μοντέλο δήλωσε ως θετικά. Προκύπτει από τον τύπο: **TP/ (TP + FP)**. Αν εφαρμόσουμε τον τύπο στα κατάλληλα δεδομένα των πινάκων σύγχυσης προκύπτουν οι τιμές που εφμανίζονται στο πεδίο  Precision του classification report για την αντίστοιχη κλάση. \n",
        "\n",
        "\n",
        "* **Recall**: Δηλώνει τι ποσοστό από τα positives για την κάθε κλάση προβλέψαμε σωστά. Προκύπτει από τον τύπο: TP /(TP + FN). Αν εφαρμόσουμε τον τύπο στα κατάλληλα δεδομένα των πινάκων σύγχυσης προκύπτουν οι τιμές που εφμανίζονται στο πεδίο  Recall του classification report για την αντίστοιχη κλάση. \n",
        "\n",
        "\n",
        "* **f1 -score**: Δηλώνει τι ποσοστό των θετικών προβλέψεων, για κάθε κλάση, ήταν σωστά. Προκύπτει από τον τύπο :\n",
        "\n",
        "    F1 Score = 2*(Recall * Precision) / (Recall + Precision)    \n",
        "και αν εφαρμόσουμε τις τιμές που προέκψαν προηγουμένως καταλήγουμε στα ίδια αποτελέσματα που υπάρχουν στo classification report, για την αντίστοιχη κλάση.\n",
        "\n",
        "Μελετώντας τις τιμές για τους τρεις classifiers:\n",
        "\n",
        "* Για τον **dummy**: Τα αποτελέσματα δεν ήταν ικανοποιητικά. Η μέθοδος που χρησιμοποιείται από default στον dummy classifier ονομάζεται \"stratified\" και για την αντιστοίχιση των κλάσεων λαμβάνει υπόψιν μόνο τη διατήρηση της ίδιας κατανομής των κλάσεων με αυτή στο train data. Σε κάθε επανάληψη του predict τα αποτελέσματα παρουσιάζουν μεταβολές, ωστόσο σε κάθε περίπτωση οι όλοι δείκτες κινούνται σε αναμενόμενες τιμές. Συγκεκριμένα, είναι λογικό να προβλέπει σταθερά περισσότερα *rejected*, δεδομένου ότι στο αρχικό dataset η κατανομή rejected-granted ήταν περίπου 55%- 45% και με ένα τυχαίο split η κατανομή στο train dataset θα είναι περίπου η ίδια. \n",
        "\n",
        "* Για τον **Naive Gaussain Bayes**: Συγκέτρωσε πολλά δείγματα στην κύρια διαγώνιο του confusion matrix.\n",
        "\n",
        "* Για τον **k-NN**: Τα αποτελέσματά του ήταν καλύτερα από τον \"dummy\", αλλά χειρότερα από τον gnb. Σημείωσε μετριο precision και σχετικά καλό Recall για το rejected, αλλά κακό για το Granted. Μια πιθανή εξήγηση σε αυτό θα ήταν ότι μπορεί να υπάρχουν συγκεκριμένα κριτήρια απόρριψης για την πίστωση, οπότε τα διανύσματα των δειγμάτων θα ήταν κοντινά. Αντίθετα, για την έγκριση πίστωσης μπορεί να είναι περισσότεροι οι παράγοντες και τα διανύσματα των δειγμάτων πολύ διαφορετικά. Επομένως, με μεγαλύτερο k, μπορεί να είχαμε μεγαλύτερο Recall για την κλάση granted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLoHK9vJsY83"
      },
      "source": [
        "##Βελτιστοποίηση ταξινομητών\n",
        "\n",
        "Ασχολούμαστε με τον κάθε ταξινομητή ξεχωριστά:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTD6qczYeoP2"
      },
      "source": [
        "###Mετρική απόδοσης: **f1-macro**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfUzT2bL0heM"
      },
      "source": [
        "def getResults(scores, macros):\n",
        "    best_estimator = scores['best_estimator'][macros[0][0]]\n",
        "    print(\"Best estimator is: \", best_estimator)\n",
        "    # confusion matrix\n",
        "    disp1 = plot_confusion_matrix(best_estimator, final_test, test_labels,\n",
        "                                        display_labels=[\"rejected\", \"granted\"],\n",
        "                                        cmap=plt.cm.Blues)\n",
        "    plt.show()\n",
        "\n",
        "    # print macros\n",
        "    print(\"The average f1-micro average is: \", scores['micro'][macros[0][0]])\n",
        "    print(\"The average f1-macro average is: \", scores['macro'][macros[0][0]])\n",
        "    print(\"Fit time: \", scores['fit_time'][macros[0][0]])\n",
        "    print(\"Predict time: \", scores['predict_time'][macros[0][0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYeTjRtIs3vw"
      },
      "source": [
        "####Dummy Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7dQhnfltaC1"
      },
      "source": [
        "# dict that maps the transformer with its model\n",
        "transformers = {'selector': selector,\n",
        "         'scaler': scaler,\n",
        "         'min_max_scaler': min_max_scaler,\n",
        "         'ros': ros,\n",
        "         'rus': rus,\n",
        "         'pca': pca\n",
        "        }\n",
        "\n",
        "\n",
        "f1_scores = runEstimators(my_transformers=transformers, my_classifiers={'dummy': dummy}, train=final_train, test=final_test, train_labels=train_labels, test_labels=test_labels)\n",
        "macros = [(k,v) for k, v in sorted(f1_scores['macro'].items(), key=lambda item: item[1], reverse=True)]\n",
        "print(macros)\n",
        "getResults(f1_scores, macros)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNphu5l_nS6y"
      },
      "source": [
        "Παρατηρούμε ότι με βάση τη μετρική απόδοσης f1-macro η βέλτιστη λύση που προτείνει ο εκτιμητής dummy Classifier είναι ΧΡΗΣΗ ΣΤΡΑΓΙΚΗΣ UNIFORM ΚΑΙ ΚΑΘΟΛΟΥ ΠΡΟΕΠΕΞΕΡΓΑΣΙΑ ΤΩΝ ΔΕΔΟΜΕΝΩΝ.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgzHSsCIe5EF"
      },
      "source": [
        "####Gaussian Naive Bayes Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH4pUSk9wWqW"
      },
      "source": [
        "f1_scores_gnb = runEstimators(my_transformers=transformers, my_classifiers={'gnb': gnb}, train=final_train, test=final_test, train_labels=train_labels, test_labels=test_labels)\n",
        "macros_gnb = [(k,v) for k, v in sorted(f1_scores_gnb['macro'].items(), key=lambda item: item[1], reverse=True)]\n",
        "print(macros_gnb)\n",
        "getResults(f1_scores_gnb, macros_gnb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lj6UMINfD9T"
      },
      "source": [
        "####k-Nearest Neighbors  Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzUe0cWKxUI4"
      },
      "source": [
        "f1_scores_knn = runEstimators(my_transformers=transformers, my_classifiers={'kNN': knn}, train=final_train, test=final_test, train_labels=train_labels, test_labels=test_labels)\n",
        "macros_knn = [(k,v) for k, v in sorted(f1_scores_knn['macro'].items(), key=lambda item: item[1], reverse=True)]\n",
        "print(macros_knn)\n",
        "getResults(f1_scores_knn, macros_knn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiM5ViNO4pTp"
      },
      "source": [
        "ΧΡΟΝΟΙ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHkGm1Pk4qto"
      },
      "source": [
        "def printRunTimes(dummy_scores, gnb_scores, knn_scores, dummy_metrics, gnb_metrics, knn_metrics):\n",
        "\n",
        "    values = [[dummy_scores['fit_time'][dummy_metrics[0][0]], gnb_scores['fit_time'][gnb_metrics[0][0]], knn_scores['fit_time'][knn_metrics[0][0]]],\n",
        "              [dummy_scores['predict_time'][dummy_metrics[0][0]], gnb_scores['predict_time'][gnb_metrics[0][0]], knn_scores['predict_time'][knn_metrics[0][0]]]]\n",
        "\n",
        "    tmp = [sum(i) for i in np.array(values).T]\n",
        "\n",
        "    values.append(tmp)\n",
        "\n",
        "    cols = ['Dummy', 'GNB', 'KNN']\n",
        "    df = pd.DataFrame(values, columns=cols, index=['Fit Time', 'Predict Time', 'Total Time'])\n",
        "    print(df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYr3gGzIDYva"
      },
      "source": [
        "printRunTimes(f1_scores, f1_scores_gnb, f1_scores_knn, macros , macros_gnb, macros_knn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V7EyPyd8308"
      },
      "source": [
        "barplot for every classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K37aXy9j87oF"
      },
      "source": [
        "# plot for f1-macro average\n",
        "def plotMetrics(dummy_scores, gnb_scores, knn_scores, dummy_metrics, gnb_metrics, knn_metrics, metric):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_axes([0,0,1,1])\n",
        "    clfs = ['dummy', 'gnb', 'knn']\n",
        "    f1_macro_scores = [dummy_scores[metric][dummy_metrics[0][0]], gnb_scores[metric][gnb_metrics[0][0]], knn_scores[metric][knn_metrics[0][0]]]\n",
        "    ax.bar(clfs,f1_macro_scores, color='cyan', width=0.3)\n",
        "    plt.title(\"F1 \"+metric+\" Average\")\n",
        "    plt.xlabel(\"Classifiers\")\n",
        "    plt.ylabel(\"F1 \"+metric+\" Score\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "plotMetrics(f1_scores, f1_scores_gnb, f1_scores_knn, macros , macros_gnb, macros_knn, \"macro\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeQeQiHsHgjr"
      },
      "source": [
        "μεταβολή της επίδοσης των ταξινομητών πριν και μετά τη βελτιστοποίησή τους.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24tFozITHjB4"
      },
      "source": [
        "# dummy\n",
        "def printPerformanceProgress(dummy_scores, gnb_scores, knn_scores, dummy_metrics, gnb_metrics, knn_metrics, default_scores, metric):\n",
        "\n",
        "    values = [\n",
        "              [default_scores[metric]['dummy1'], default_scores[metric]['gnb2'], default_scores[metric]['kNN3']],\n",
        "              [dummy_scores[metric][dummy_metrics[0][0]], gnb_scores[metric][gnb_metrics[0][0]], knn_scores[metric][knn_metrics[0][0]]]\n",
        "              ]\n",
        "\n",
        "\n",
        "    cols = ['Dummy', 'GNB', 'KNN']\n",
        "    df = pd.DataFrame(values, columns=cols, index=['Default', 'Optimized'])\n",
        "    print(df)\n",
        "\n",
        "\n",
        "\n",
        "printPerformanceProgress(f1_scores, f1_scores_gnb, f1_scores_knn, macros , macros_gnb, macros_knn, f1_scores_default, 'macro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHC8hOq7_FfM"
      },
      "source": [
        "###Μετρική απόδοσης **f1-micro**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRk4IlHDfOyy"
      },
      "source": [
        "####Dummy Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0DYeRc-_a-K"
      },
      "source": [
        "f1_scores_micro = runEstimators(my_transformers=transformers, my_classifiers={'dummy': dummy}, scoring='f1_micro', train=final_train, test=final_test, train_labels=train_labels, test_labels=test_labels)\n",
        "micros = [(k,v) for k, v in sorted(f1_scores['micro'].items(), key=lambda item: item[1], reverse=True)]\n",
        "print(micros)\n",
        "getResults(f1_scores_micro, micros)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZWcG-JFfW6i"
      },
      "source": [
        "####Gaussian Naive Bayes Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqT7gvNTAoPa"
      },
      "source": [
        "f1_scores_gnb_micro = runEstimators(my_transformers=transformers, my_classifiers={'gnb': gnb}, scoring='f1_micro', train=final_train, test=final_test, train_labels=train_labels, test_labels=test_labels)\n",
        "micros_gnb = [(k,v) for k, v in sorted(f1_scores_gnb['micro'].items(), key=lambda item: item[1], reverse=True)]\n",
        "# print(micros_gnb)\n",
        "getResults(f1_scores_gnb_micro, micros_gnb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf4VsNHQfbfz"
      },
      "source": [
        "####k-Nearest Neighbors  Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3AI3X2zByce"
      },
      "source": [
        "f1_scores_knn_micro = runEstimators(my_transformers=transformers, my_classifiers={'kNN': knn}, scoring='f1_micro', train=final_train, test=final_test, train_labels=train_labels, test_labels=test_labels)\n",
        "micros_knn = [(k,v) for k, v in sorted(f1_scores_knn['micro'].items(), key=lambda item: item[1], reverse=True)]\n",
        "# print(micros_knn)\n",
        "getResults(f1_scores_knn_micro, micros_knn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq1vmKQ0EqfJ"
      },
      "source": [
        "printRunTimes(f1_scores_micro, f1_scores_gnb_micro, f1_scores_knn_micro, micros , micros_gnb, micros_knn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_V95MaB9GP7J"
      },
      "source": [
        "plotMetrics(f1_scores_micro, f1_scores_gnb_micro, f1_scores_knn_micro, micros , micros_gnb, micros_knn, \"micro\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knXJgKi3LgSL"
      },
      "source": [
        "printPerformanceProgress(f1_scores_micro, f1_scores_gnb_micro, f1_scores_knn_micro, micros , micros_gnb, micros_knn, f1_scores_default, 'micro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tNVM0Z73x5-"
      },
      "source": [
        "Εκτελούμε τις παρακάτω εντολές για να βρούμε τη διακύμανση στις τιμές των διαφόρων χαρακτηριστικών:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZUXF8pHpiHE"
      },
      "source": [
        "# train_variance = train.var(axis=0)\n",
        "# print(train_variance)\n",
        "# print(np.max(train_variance))\n",
        "# print(train_variance.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E93Uww5vMf_9"
      },
      "source": [
        "#Μεγάλο Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_UcSR06YtXm"
      },
      "source": [
        "Αρχικά κατεβάζουμε το συμπιεσμένο αρχείο, το αποθηκεύουμε ως data.zip και στη συνέχεια το αποσυμπιέζουμε, οπότε παράγονται 5 αρχείο τύπου .arff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UTSjkZzGS01"
      },
      "source": [
        "download(\"http://archive.ics.uci.edu/ml/machine-learning-databases/00365/data.zip\", 'data.zip')\n",
        "!unzip data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VT06Xa8Y7c1"
      },
      "source": [
        "Για να μετατρέψουμε τα αρχεία σε .csv, αρκεί να κρατήσουμε μόνο τις γραμμές που δεν ξεκινάνε με “%”, “@” και δεν είναι κενές.\n",
        "Επομένως, για καθένα από τα .arff αρχεία εκτελούμε την εντολή:\n",
        "```\n",
        "cat ${file} | grep -ve \"^@\\|^%\" | grep -v \"^[[:space:]]*$\" >> data.csv\n",
        "```\n",
        "Η εντολή αυτή θα τυπώσει μόνο τις επιθυμητές γραμμές και θα ανακατευθύνει το αποτέλεσμα στο αρχείο data.csv, προσθέτοντάς το στο τέλος του.\n",
        "\n",
        "Για να πραγματοποιήσουμε αυτήν τη διαδικασία για όλα τα .arff εκτελούμε τα παρακάτω:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL6_s3nxQjZH"
      },
      "source": [
        "%%shell\n",
        "truncate -s 0 data.csv  #create or empty the data.csv file\n",
        "for file in ./*.arff    #for every .arff file in the current directory\n",
        "do\n",
        "    echo ${file}        #print file name for checking the progress purposes\n",
        "    cat ${file} | grep -ve \"^@\\|^%\" | grep -v \"^[[:space:]]*$\" >> data.csv  #print the lines with the samples in the end of data.csv file\n",
        "done\n",
        "wc -l data.csv          #check the length of the final file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7GN32b9XOkW"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "big_df = pd.read_csv(\"data.csv\", header=None)\n",
        "# print df\n",
        "big_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxeKdpKDW9qR"
      },
      "source": [
        "##Βασικές Πληροφορίες"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xUuWwFzYn9S"
      },
      "source": [
        "###Πληροφορίες dataset\n",
        "Το dataset, πρόκειται επομένως για ένα σύνολο **43405 δειγμάτων**, με κάθε δείγμα να έχει **64 χαρακτηριστικά**. Η τελευταία στήλη είναι η κλάση στην οποία ανήκει το δείγμα, δηλαδή δηλώνει αν πτώχευσε (0) ή όχι (1) η συγκεκριμένη εταιρεία.\n",
        "\n",
        "Πιο συγκεκριμένα, το dataset *Polish companies bankruptcy* αποτελεί ένα σύνολο δεδομένων σχετικά με πολωνικές εταιρείες που πτώχευσαν ή όχι παραθέτοντας αρκετά (64) χαρακτηριστικά τους. Τα στοιχεία έχουν συλλεγεί ως 5 διαφορετικά σύνολα που το καθένα αντιστοιχεί σε 5 διαφορετικά έτη. Παρολαυτά στα πλαίσια της παρούσας εργασίας τα 5 σύνολα δεδομένων θα αντιμετωπιστούν ως ένα ενιαίο σύνολο δεδομένων.\n",
        "\n",
        "Παρακάτω παρατίθεται μία σύντομη περιγραφή του καθενός από τα χαρακτηριστικά των δειγμάτων που υπάρχουν στο dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuRmRk3SXd9y"
      },
      "source": [
        "| Attribute | Description                                                                                                         |\n",
        "|-----------|---------------------------------------------------------------------------------------------------------------------|\n",
        "| X1        | net profit / total assets                                                                                           |\n",
        "| X2        | total liabilities / total assets                                                                                    |\n",
        "| X3        | working capital / total assets                                                                                      |\n",
        "| X4        | current assets / short-term liabilities                                                                             |\n",
        "| X5        | [(cash + short-term securities + receivables - short-term liabilities) / (operating expenses - depreciation)] * 365 |\n",
        "| X6        | retained earnings / total assets                                                                                    |\n",
        "| X7        | EBIT / total assets                                                                                                 |\n",
        "| X8        | book value of equity / total liabilities                                                                            |\n",
        "| X9        | sales / total assets                                                                                                |\n",
        "| X10       | equity / total assets                                                                                               |\n",
        "| X11       | (gross profit + extraordinary items + financial expenses) / total assets                                            |\n",
        "| X12       | gross profit / short-term liabilities                                                                               |\n",
        "| X13       | (gross profit + depreciation) / sales                                                                               |\n",
        "| X14       | (gross profit + interest) / total assets                                                                            |\n",
        "| X15       | (total liabilities * 365) / (gross profit + depreciation)                                                           |\n",
        "| X16       | (gross profit + depreciation) / total liabilities                                                                   |\n",
        "| X17       | total assets / total liabilities                                                                                    |\n",
        "| X18       | gross profit / total assets                                                                                         |\n",
        "| X19       | gross profit / sales                                                                                                |\n",
        "| X20       | (inventory * 365) / sales                                                                                           |\n",
        "| X21       | sales (n) / sales (n-1)                                                                                             |\n",
        "| X22       | profit on operating activities / total assets                                                                       |\n",
        "| X23       | net profit / sales                                                                                                  |\n",
        "| X24       | gross profit (in 3 years) / total assets                                                                            |\n",
        "| X25       | (equity - share capital) / total assets                                                                             |\n",
        "| X26       | (net profit + depreciation) / total liabilities                                                                     |\n",
        "| X27       | profit on operating activities / financial expenses                                                                 |\n",
        "| X28       | working capital / fixed assets                                                                                      |\n",
        "| X29       | logarithm of total assets                                                                                           |\n",
        "| X30       | (total liabilities - cash) / sales                                                                                  |\n",
        "| X31       | (gross profit + interest) / sales                                                                                   |\n",
        "| X32       | (current liabilities * 365) / cost of products sold                                                                 |\n",
        "| X33       | operating expenses / short-term liabilities                                                                         |\n",
        "| X34       | operating expenses / total liabilities                                                                              |\n",
        "| X35       | profit on sales / total assets                                                                                      |\n",
        "| X36       | total sales / total assets                                                                                          |\n",
        "| X37       | (current assets - inventories) / long-term liabilities                                                              |\n",
        "| X38       | constant capital / total assets                                                                                     |\n",
        "| X39       | profit on sales / sales                                                                                             |\n",
        "| X40       | (current assets - inventory - receivables) / short-term liabilities                                                 |\n",
        "| X41       | total liabilities / ((profit on operating activities + depreciation) * (12/365))                                    |\n",
        "| X42       | profit on operating activities / sales                                                                              |\n",
        "| X43       | rotation receivables + inventory turnover in days                                                                   |\n",
        "| X44       | (receivables * 365) / sales                                                                                         |\n",
        "| X45       | net profit / inventory                                                                                              |\n",
        "| X46       | (current assets - inventory) / short-term liabilities                                                               |\n",
        "| X47       | (inventory * 365) / cost of products sold                                                                           |\n",
        "| X48       | EBITDA (profit on operating activities - depreciation) / total assets                                               |\n",
        "| X49       | EBITDA (profit on operating activities - depreciation) / sales                                                      |\n",
        "| X50       | current assets / total liabilities                                                                                  |\n",
        "| X51       | short-term liabilities / total assets                                                                               |\n",
        "| X52       | (short-term liabilities * 365) / cost of products sold)                                                             |\n",
        "| X53       | equity / fixed assets                                                                                               |\n",
        "| X54       | constant capital / fixed assets                                                                                     |\n",
        "| X55       | working capital                                                                                                     |\n",
        "| X56       | (sales - cost of products sold) / sales                                                                             |\n",
        "| X57       | (current assets - inventory - short-term liabilities) / (sales - gross profit - depreciation)                       |\n",
        "| X58       | total costs /total sales                                                                                            |\n",
        "| X59       | long-term liabilities / equity                                                                                      |\n",
        "| X60       | sales / inventory                                                                                                   |\n",
        "| X61       | sales / receivables                                                                                                 |\n",
        "| X62       | (short-term liabilities *365) / sales                                                                               |\n",
        "| X63       | sales / short-term liabilities                                                                                      |\n",
        "| X64       | sales / fixed assets                                                                                                |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPGd-9K_F3QC"
      },
      "source": [
        "###Περιγραφή χαρακτηριστικών του dataset\n",
        "\n",
        "Όπως είδαμε και προηγουμένως το σύνολο περιλαμβάνει **43405 δείγματα**, με κάθε δείγμα να έχει **64 χαρακτηριστικά**. **Όλα τα χαρακτηριστικά παίρνουν αριθμητικές, μη διατεταγμένες τιμές**.\n",
        "\n",
        "Επίσης, το dataset **δεν** περιλαμβάνει επικεφαλίδες και αρίθμηση γραμμών.\n",
        "\n",
        "Οι ετικέτες των κλάσεων, χρεωκοπεία ή όχι, **αναπαριστόνται με τιμές 0 και 1 αντίστοιχα και βρίσκονται στην τελευταία στήλη**.\n",
        "\n",
        "Οι μοναδικές αλλαγές των αρχικών αρχείων .arff που χρειάστηκε να γίνουν είναι η μετατροπή τους σε αρχεία .csv, που έγινε με όπως περιγράφηκε παραπάνω. Τέλος, τα δεδομένα από τα 5 συνολικά αρχεία συγκεντρώθηκαν σε ένα μόνο αρχείο, το **data.csv**, όπως περιγράφηκε παραπάνω."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgoZJWICOWVY"
      },
      "source": [
        "###Εντοπισμός απουσιάζουσων τιμών χαρακτηριστικών\n",
        "\n",
        "Στη συνέχεια, θα εντοπίσουμε τις απουσιάζουσες τιμές. Αυτές δηλώνονται στο dataset με τον χαρακτήρα \"?\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a93epkQkOaVq"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# replace \"?\" with np.NaN\n",
        "big_df.replace('?',np.NaN,inplace=True)\n",
        "\n",
        "# calculate the number of samples with at least one missing attribute\n",
        "# big_df.isna() is the mask of big_df where each element is True if is NaN\n",
        "# num_of_incomplete_samples_big is the length of the list of samples that have at least one True(NaN)\n",
        "num_of_incomplete_samples_big = len([i for i in np.array(big_df.isna()) if True in i])\n",
        "\n",
        "print(\"The samples of the dataset that have at least one missing attribute are \", num_of_incomplete_samples_big)\n",
        "print(\"Which means that the \", num_of_incomplete_samples_big*100/big_df.shape[0], \"% of the samples have missing values.\", sep=\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UxusBvHPVMq"
      },
      "source": [
        "Σύμφωνα με τα αποτελέσματα, το **53.99% των δειγμάτων έχουν απουσιάζουσες τιμές**.\n",
        "\n",
        "Βλέπουμε, λοιπόν ότι ένα πολύ μεγάλο ποσοστό των δειγμάτων έχουν τουλάχιστον μία απουσιάζουσα τιμή, επομένως η διαγραφή των δειγμάτων με απουσιάζουσες τιμές είναι απαγορευτική.\n",
        "\n",
        "Στη συνέχεια, θα ελέγξουμε πόσες απουσιάζουσες τιμές έχει το κάθε χαρακτηριστικό:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1zBGJGSPz7y"
      },
      "source": [
        "# the list of the number of missing values for each attribute occurs by \n",
        "# summing the elements of the inverse of df, without df's last column.\n",
        "# df's last column is the class attribute and it is alwasy present.\n",
        "incomplete_attrs_big = [sum(i) for i in np.array(big_df.isna())[:,:big_df.shape[1]-1].T]\n",
        "print(\"For each attribute of the dataset, the number of the missing values is\")\n",
        "print(incomplete_attrs_big)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbFJ_U_bQNMe"
      },
      "source": [
        "# list of all attributes with more than 30% of their values missing\n",
        "incomplete_attributes = [i for i, x in enumerate(incomplete_attrs_big) if x/big_df.shape[0] > 0.3]\n",
        "print(incomplete_attributes)\n",
        "\n",
        "for x in incomplete_attributes:\n",
        "    print(incomplete_attrs_big[x]*100/big_df.shape[0], \"%\")\n",
        "# print(incomplete_attrs_big[incomplete_attribute]*100/big_df.shape[0], \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ulSghZPQe_-"
      },
      "source": [
        "Παρατηρούμε ότι το 43.7% των τιμών του χαρακτηριστικού 37ου (η αρίθμηση ξεκινάει από το 0) απουσιάζει. Με βάση αυτήν την παρατήρηση θα ήταν μαλλον καλύτερο να διαγραφεί εντελώς αυτή η στήλη ώστε να μην επηρεάσει αρνητικά.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KghVrIMSdXV"
      },
      "source": [
        "big_df = big_df.drop(big_df.columns[[incomplete_attribute]], axis=1)\n",
        "big_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8QLm3_xVMIM"
      },
      "source": [
        "Για τις υπόλοιπες τιμές, θα αντικαταστήσουμε κάθε απουσιάζουσα τιμή χαρακτηριστικού με τη μέση τιμή. Αυτό θα γίνει μετά τον διαχωρισμό του dataset σε train και test και πριν την έναρξη του Cross Validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL5TGc3BXKkL"
      },
      "source": [
        "###Κατανομή των κλάσεων\n",
        "\n",
        "Στη συνέχεια, θα εξετάσουμε τι κατανομή έχουν οι δύο κλάσεις στα δεδομένα:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRcFckz5Yvky"
      },
      "source": [
        "num_of_rows = big_df.shape[0]\n",
        "num_of_attrs = big_df.shape[1] - 1 #remove one element because of the class attribute\n",
        "\n",
        "# get labesl and features\n",
        "labels_df = big_df.iloc[:, [num_of_attrs]] # τα labels είναι στην τελευταία κολώνα\n",
        "features_df = big_df.iloc[:, 0:num_of_attrs]  # τα features είναι όλες οι προηγούμενες κολώνες\n",
        "\n",
        "labels = labels_df.values.reshape(num_of_rows,)\n",
        "features = features_df.values\n",
        "# convert to int\n",
        "labels.astype(int)\n",
        "labels = np.array(labels, dtype='int64')\n",
        "\n",
        "\n",
        "# find how many of each class\n",
        "bin_count = np.bincount(labels)\n",
        "print (\"bincount:\", bin_count)\n",
        "print(sum(bin_count))\n",
        "print(\"The percentage of 0's in data: \", bin_count[0]*100/sum(bin_count), \"%.\")\n",
        "print(\"The percentage of 1's in data: \", bin_count[1]*100/sum(bin_count), \"%.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jayK-hD4aYUM"
      },
      "source": [
        "Παρατηρούμε ότι το **95%** των δειγμάτων ανήκουν στην κλάση **0**, ενώ μόλις το **4.81%** στην κλάση **1**. Επομένως, **το dataset είναι εντελώς ανισόρροπο**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2dKrxCuWiI6"
      },
      "source": [
        "###Διαχωρισμός του dataset\n",
        "Διαχωρίζουμε το dataset σε train και test set, χρησιμοποιώντας το 70% των δεδομένων για το training και το 30% για το testing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo_kT2rvd3rc"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "train, test, train_labels, test_labels = train_test_split(features, labels, test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oYFXJiPd6gP"
      },
      "source": [
        "train_pd = pd.DataFrame(data=train[:,:],    # values\n",
        "                 index=train[:,0])    # 1st column as index\n",
        "                  \n",
        "test_pd = pd.DataFrame(data=test[:,:],    # values\n",
        "                 index=test[:,0])    # 1st column as index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4YX9wBleC_m"
      },
      "source": [
        "print(train.shape)\n",
        "print(train_pd.shape)\n",
        "\n",
        "print(test.shape)\n",
        "print(test_pd.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_L1wklKeT4r"
      },
      "source": [
        "##Ταξινόμηση"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQnP7OCAosAV"
      },
      "source": [
        "Θα χρησιμποιηθούν οι ίδιες συναρτήσεις που χρησιμοποιήθηκαν στο μικρό Dataset, αφού το μόνο που αρκεί είναι η κλήση τους με τις κατάλληλες παραμέτρους.\n",
        "\n",
        "Αρχικά, όμως θα αντικαταστήσουμε τις απουσιάζουσες με τον μέσο όρο των τιμών στο αντίστοιχο χαρακτηριστικό:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2qWZ_rEqdVl"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# create imputer that will replace NaN with the most frequent value\n",
        "imp=SimpleImputer(missing_values=np.NaN,strategy=\"mean\")\n",
        "# fit and transform train data by replacing NaN with the most frequent value of the attribute\n",
        "i_train=pd.DataFrame(imp.fit_transform(train_pd))\n",
        "i_train.columns=train_pd.columns\n",
        "i_train.index=train_pd.index\n",
        "\n",
        "# transform the test data using the same model\n",
        "i_test = imp.transform(test_pd.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D69o3iBxqzsd"
      },
      "source": [
        "Κατηγορικές μεταβλητές δεν υπάρχουν στο dataset, επομένως μπορούμε να εισάγουμε τα δεδομένα στο Pipeline και να δοκιμάσουμε τους διάφορους ταξινομητές που προκύπτουν."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u28JfWCHpG8E"
      },
      "source": [
        "###Baseline Classification - Ταξινόμηση χωρίς προεπεξεργασία"
      ]
    }
  ]
}